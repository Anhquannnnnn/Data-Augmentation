





import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from typing import List, Union, Optional
import time
from datetime import datetime, timedelta

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")





data = pd.DataFrame({
    'age': [5,4,5,4,1,2,5,8,8,7,4,1,1,5,74,7,8,7,4,1],
    'weight': [20,21,24,10,2,12,15,15,15,12,15,10,20,30,174,15,18,16,1,15],
    'height': [10,15,1,10,174,485,505,256,45,5454,545,14,125,214,256,24,125,145,125,125],
})











class DataAnalyzer:
    """
    A class for comprehensive data analysis including summary statistics,
    visualizations, and statistical tests.
    """
    
    def __init__(self, data: pd.DataFrame):
        """
        Initialize the DataAnalyzer with a pandas DataFrame.
        
        Parameters:
        -----------
        data : pd.DataFrame
            The input data to analyze
        """
        self.data = data
        self.numeric_columns = data.select_dtypes(include=[np.number]).columns
        self.categorical_columns = data.select_dtypes(exclude=[np.number]).columns
        
    def get_summary_statistics(self) -> pd.DataFrame:
        """
        Generate comprehensive summary statistics for numeric columns.
        """
        summary = self.data[self.numeric_columns].describe()
        # Add additional statistics
        summary.loc['skewness'] = self.data[self.numeric_columns].skew()
        summary.loc['kurtosis'] = self.data[self.numeric_columns].kurtosis()
        return summary
    
    def analyze_distributions(self, columns: Optional[List[str]] = None) -> dict:
        """
        Perform normality tests and distribution analysis.
        
        Parameters:
        -----------
        columns : List[str], optional
            Specific columns to analyze. If None, analyzes all numeric columns.
        """
        if columns is None:
            columns = self.numeric_columns
            
        results = {}
        for col in columns:
            if col in self.numeric_columns:
                # Shapiro-Wilk test for normality
                shapiro_stat, shapiro_p = stats.shapiro(self.data[col].dropna())
                
                # Kolmogorov-Smirnov test
                ks_stat, ks_p = stats.kstest(
                    self.data[col].dropna(), 
                    'norm',
                    args=(self.data[col].mean(), self.data[col].std())
                )
                
                results[col] = {
                    'shapiro_test': {'statistic': shapiro_stat, 'p_value': shapiro_p},
                    'ks_test': {'statistic': ks_stat, 'p_value': ks_p}
                }
        
        return results
    
    def chi_squared_test(self, col1: str, col2: str) -> dict:
        """
        Perform Chi-squared test of independence between two categorical variables.
        """
        contingency_table = pd.crosstab(self.data[col1], self.data[col2])
        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
        
        return {
            'chi2_statistic': chi2,
            'p_value': p_value,
            'degrees_of_freedom': dof,
            'contingency_table': contingency_table,
            'expected_frequencies': pd.DataFrame(
                expected,
                index=contingency_table.index,
                columns=contingency_table.columns
            )
        }
    
    def correlation_analysis(self) -> pd.DataFrame:
        """
        Calculate correlation matrix for numeric columns.
        """
        return self.data[self.numeric_columns].corr()
    
    def plot_distributions(self, columns: Optional[List[str]] = None):
        """
        Create distribution plots for specified columns.
        """
        if columns is None:
            columns = self.numeric_columns
            
        n_cols = min(2, len(columns))
        n_rows = (len(columns) + 1) // 2
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))
        axes = axes.flatten() if n_rows * n_cols > 1 else [axes]
        
        for idx, col in enumerate(columns):
            if col in self.numeric_columns:
                # Histogram with KDE
                sns.histplot(data=self.data, x=col, kde=True, ax=axes[idx])
                axes[idx].set_title(f'Distribution of {col}')
                
        plt.tight_layout()
        return fig
    
    def plot_correlation_heatmap(self):
        """
        Create a correlation heatmap for numeric columns.
        """
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            self.correlation_analysis(),
            annot=True,
            cmap='coolwarm',
            center=0,
            vmin=-1,
            vmax=1
        )
        plt.title('Correlation Heatmap')
        return plt.gcf()
    
    def plot_boxplots(self, columns: Optional[List[str]] = None):
        """
        Create boxplots for specified columns.
        """
        if columns is None:
            columns = self.numeric_columns
            
        plt.figure(figsize=(12, 6))
        self.data[columns].boxplot()
        plt.xticks(rotation=45)
        plt.title('Boxplots of Numeric Variables')
        return plt.gcf()

    def generate_report(self) -> str:
        """
        Generate a comprehensive analysis report.
        """
        report = []
        report.append("Data Analysis Report")
        report.append("===================")
        
        # Basic information
        report.append("\n1. Dataset Overview")
        report.append(f"Number of rows: {len(self.data)}")
        report.append(f"Number of columns: {len(self.data.columns)}")
        report.append(f"Missing values:\n{self.data.isnull().sum().to_string()}")
        
        # Summary statistics
        report.append("\n2. Summary Statistics")
        report.append(self.get_summary_statistics().to_string())
        
        # Distribution analysis
        report.append("\n3. Distribution Analysis")
        dist_results = self.analyze_distributions()
        for col, tests in dist_results.items():
            report.append(f"\n{col}:")
            report.append(f"Shapiro-Wilk test p-value: {tests['shapiro_test']['p_value']:.4f}")
            report.append(f"Kolmogorov-Smirnov test p-value: {tests['ks_test']['p_value']:.4f}")
        
        return "\n".join(report)





df = pd.DataFrame(data)
analyzer = DataAnalyzer(df)

# Get summary statistics
summary_stats = analyzer.get_summary_statistics()
#display(summary_stats)
# Analyze distributions
distribution_analysis = analyzer.analyze_distributions()

# Create visualizations
analyzer.plot_distributions()
analyzer.plot_correlation_heatmap()
analyzer.plot_boxplots()

# Generate full report
report = analyzer.generate_report()








class TabularDataset(Dataset):
    def __init__(self, data):
        self.data = torch.tensor(data,dtype=torch.float32)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

class TVAE(nn.Module):
    def __init__(self, input_dim, hidden_dims : list, latent_dim=32,use_batch_norm=True, dropout_rate = 0.1 ):
        super(TVAE, self).__init__()
        self.latent_dim = latent_dim
        self.input_dim = input_dim
        self.dropout_rate = dropout_rate
        # Encoder
        pre_dim = self.input_dim
        encoder_layers = []
        for dim in hidden_dims:
            encoder_layers.append(nn.Linear(pre_dim, dim))
            if use_batch_norm == True:
                encoder_layers.append(nn.BatchNorm1d(dim))
            encoder_layers.extend([nn.LeakyReLU(0.2),
                                   nn.Dropout(dropout_rate),
                                  ])
            pre_dim = dim
        self.encoder = nn.Sequential(*encoder_layers)
       
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)
        
        # Decoder
        pre_dim = latent_dim
        decoder_layers = []
        for dim in reversed(hidden_dims):
            decoder_layers.append(nn.Linear(pre_dim, dim))
            if use_batch_norm == True:
                encoder_layers.append(nn.BatchNorm1d(dim))
            encoder_layers.extend([nn.LeakyReLU(0.2),
                                   nn.Dropout(dropout_rate),
                                  ])
            pre_dim = dim
        decoder_layers.extend([
            nn.Linear(hidden_dims[0], input_dim),
            nn.Tanh() 
        ])
        
        self.decoder = nn.Sequential(*decoder_layers)
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                module.bias.data.fill_(0.01)
    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_var(h)
    
    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var
        
    def generate(self, n_samples, device='cpu'):
        """Generate synthetic samples from random latent vectors"""
        with torch.no_grad():
            z = torch.randn(n_samples, self.latent_dim).to(device)
            return self.decode(z)
            
    def get_reconstruction_loss(self, x_recon, x, mu, log_var, beta=1.0):
        """
        Compute VAE loss with adjustable β-VAE formulation
        """
        # Reconstruction loss (MSE)
        recon_loss = F.mse_loss(x_recon, x, reduction='sum')
        
        # KL divergence loss
        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        
        # Total loss with β weighting
        total_loss = recon_loss + beta * kl_loss
        
        return {
            'total_loss': total_loss,
            'reconstruction_loss': recon_loss,
            'kl_loss': kl_loss
        }
    
    def interpolate(self, x1, x2, steps=10):
        """Interpolate between two input samples in latent space"""
        with torch.no_grad():
            # Encode both inputs
            mu1, _ = self.encode(x1.unsqueeze(0))
            mu2, _ = self.encode(x2.unsqueeze(0))
            
            # Create interpolation points
            alphas = torch.linspace(0, 1, steps)
            interpolated = []
            
            # Generate intermediate points
            for alpha in alphas:
                z = mu1 * (1 - alpha) + mu2 * alpha
                interpolated.append(self.decode(z))
                
            return torch.cat(interpolated, dim=0)
            
    def compute_metrics(self, x, x_recon):
        """Compute quality metrics for generated samples"""
        with torch.no_grad():
            # Mean absolute error
            mae = F.l1_loss(x_recon, x, reduction='mean')
            
            # Mean squared error
            mse = F.mse_loss(x_recon, x, reduction='mean')
            
            # Cosine similarity
            cos_sim = F.cosine_similarity(x_recon, x).mean()
            
            return {
                'mae': mae.item(),
                'mse': mse.item(),
                'cosine_similarity': cos_sim.item()
            }

def train_tvae(model, dataloader, num_epochs=100, learning_rate=1e-3):
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    start_time = time.time()
    epoch_times = []
    
    print(f"Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    for epoch in range(num_epochs):
        epoch_start = time.time()
        total_loss = 0
        for batch in dataloader:
            optimizer.zero_grad()
            
            # Forward pass
            recon_batch, mu, log_var = model(batch)

            # loss
            loss = model.get_reconstruction_loss(recon_batch,batch, mu, log_var )['total_loss']
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
      
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader.dataset):.4f}')
            epoch_end = time.time()
            epoch_duration = epoch_end - epoch_start
            epoch_times.append(epoch_duration)
    print(f"Training finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    total_duration = time.time() - start_time
    print(f"Total training time: {format_time(total_duration)}")
def generate_synthetic_data(model, n_samples, original_data):
    model.eval()
    with torch.no_grad():
        # Sample from latent space
        z = torch.randn(n_samples, model.fc_mu.out_features)
        # Generate synthetic data
        synthetic_data = model.decode(z)
    return synthetic_data.numpy()







def format_time(seconds):
    """Convert seconds to a human-readable format"""
    return str(timedelta(seconds=int(seconds)))
def main():
    # Create sample data
    np.random.seed(42)
    n_samples = 1000
    n_features = 5
    original_data = np.random.randn(n_samples, n_features)
    
    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(original_data)
    
    # Create dataset and dataloader
    dataset = TabularDataset(scaled_data)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # Initialize and train TVAE
    model = TVAE(input_dim=n_features,hidden_dims=[256,128,64,64])
    train_tvae(model, dataloader)
    
    # Generate synthetic data
    synthetic_data = generate_synthetic_data(model, n_samples, original_data)
    
    # Inverse transform the synthetic data
    synthetic_data_original_scale = scaler.inverse_transform(synthetic_data)
    
    # Convert to DataFrame
    synthetic_df = pd.DataFrame(
        synthetic_data_original_scale,
        columns=[f'feature_{i+1}' for i in range(n_features)]
    )
    
    return synthetic_df

if __name__ == "__main__":
    synthetic_df = main()
    print("\nSynthetic Data Sample:")
    print(synthetic_df.head())
    print("\nSynthetic Data Statistics:")
    print(synthetic_df.describe())


print(torch.cuda.is_available())


!conda env list



