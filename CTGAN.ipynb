{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e1a9a7b-1dfc-452a-8b98-f057b78594e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_transformer.data_sampler import DataSampler\n",
    "from data_transformer.data_transformer import DataTransformer\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "263baa92-b1c6-496d-a9f6-813d1216de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "    for _ in range(10):\n",
    "        transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "        if not torch.isnan(transformed).any():\n",
    "            return transformed\n",
    "\n",
    "class SmallRes(Module):\n",
    "    def __init__(self, d_input, d_output):\n",
    "        super().__init__()\n",
    "        self.layers = Sequential(\n",
    "            Linear(d_input,d_output),\n",
    "            BatchNorm1d(d_output),\n",
    "            LeakyReLU(0.2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return torch.cat([x, out], dim = 1)\n",
    "class Generator(Module):\n",
    "    def __init__(self, embedding_dim, generator_dim, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        seq = []\n",
    "        dim = embedding_dim\n",
    "        for index in generator_dim:\n",
    "            seq.append(SmallRes(dim,index))\n",
    "            dim += index\n",
    "        seq.append(Dropout(0.3))\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.layers = Sequential(*seq)\n",
    "                \n",
    "    def forward(self, noise, cond_vec=None):\n",
    "        if cond_vec is not None:\n",
    "            x = torch.cat([noise, cond_vec], dim=1)\n",
    "\n",
    "        else:\n",
    "            x = noise\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "class Discriminator(Module):\n",
    "    def __init__(self, input_dim, discriminator_dim, pac=10):\n",
    "        super().__init__()\n",
    "        dim = input_dim * pac\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            seq += [Linear(dim, item), BatchNorm1d(item), LeakyReLU(0.2), Dropout(0.3)]\n",
    "            dim = item\n",
    "\n",
    "        seq += [Linear(dim, 1)]\n",
    "        self.seq = Sequential(*seq)\n",
    "        \n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "\n",
    "        batch_size = real_data.size(0) // pac\n",
    "        alpha = torch.rand(batch_size, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1)).view(-1, real_data.size(1))\n",
    "        \n",
    "        interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolates.requires_grad_(True)\n",
    "        \n",
    "        disc_interpolates = self(interpolates)\n",
    "        \n",
    "        ones = torch.ones(disc_interpolates.size(), device=device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=ones,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(-1, pac * real_data.size(1))\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        gradient_penalty = lambda_ * ((gradient_norm - 1) ** 2).mean()\n",
    "        \n",
    "        return gradient_penalty\n",
    "    def forward(self, input_):\n",
    "        assert input_.size()[0] % self.pac == 0\n",
    "        return self.seq(input_.view(-1, self.pacdim))\n",
    "class CTGAN():\n",
    "    def __init__(self,\n",
    "        embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256), generator_lr=2e-4,generator_decay=1e-6, discriminator_lr=2e-4, \n",
    "        discriminator_decay=1e-6,  discriminator_steps=1, log_frequency=True, pac=10, cuda=True):\n",
    "\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._generator_dim = generator_dim\n",
    "        self._discriminator_dim = discriminator_dim\n",
    "\n",
    "        self._generator_lr = generator_lr\n",
    "        self._generator_decay = generator_decay\n",
    "        self._discriminator_lr = discriminator_lr\n",
    "        self._discriminator_decay = discriminator_decay\n",
    "\n",
    "        self._discriminator_steps = discriminator_steps\n",
    "        self._log_frequency = log_frequency\n",
    "        self.pac = pac\n",
    "\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "        self._transformer = None\n",
    "        self._data_sampler = None\n",
    "        self._generator = None\n",
    "        self.loss_values = None\n",
    "    def _apply_activate(self, data):\n",
    "        data_t = []\n",
    "        st = 0\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if span_info.activation_fn == 'tanh':\n",
    "                    ed = st + span_info.dim\n",
    "                    data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                    st = ed\n",
    "                elif span_info.activation_fn == 'softmax':\n",
    "                    ed = st + span_info.dim\n",
    "                    transformed = _gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                    data_t.append(transformed)\n",
    "                    st = ed\n",
    "                else:\n",
    "                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "\n",
    "        return torch.cat(data_t, dim=1)\n",
    "    def _cond_loss(self, data, c, m):\n",
    "        loss = []\n",
    "        st = 0\n",
    "        st_c = 0\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
    "                    st += span_info.dim\n",
    "                else:\n",
    "                    ed = st + span_info.dim\n",
    "                    ed_c = st_c + span_info.dim\n",
    "                    tmp = functional.cross_entropy(\n",
    "                        data[:, st:ed], torch.argmax(c[:, st_c:ed_c], dim=1), reduction='none'\n",
    "                    )\n",
    "                    loss.append(tmp)\n",
    "                    st = ed\n",
    "                    st_c = ed_c\n",
    "\n",
    "        loss = torch.stack(loss, dim=1)  \n",
    "\n",
    "        return (loss * m).sum() / data.size()[0]\n",
    "    def _validate_discrete_columns(self, train_data, discrete_columns):\n",
    "        if isinstance(train_data, pd.DataFrame):\n",
    "            invalid_columns = set(discrete_columns) - set(train_data.columns)\n",
    "        elif isinstance(train_data, np.ndarray):\n",
    "            invalid_columns = []\n",
    "            for column in discrete_columns:\n",
    "                if column < 0 or column >= train_data.shape[1]:\n",
    "                    invalid_columns.append(column)\n",
    "        else:\n",
    "            raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')\n",
    "\n",
    "        if invalid_columns:\n",
    "            raise ValueError(f'Invalid columns found: {invalid_columns}')\n",
    "    def _validate_null_data(self, train_data, discrete_columns):\n",
    "        if isinstance(train_data, pd.DataFrame):\n",
    "            continuous_cols = list(set(train_data.columns) - set(discrete_columns))\n",
    "            any_nulls = train_data[continuous_cols].isna().any().any()\n",
    "        else:\n",
    "            continuous_cols = [i for i in range(train_data.shape[1]) if i not in discrete_columns]\n",
    "            any_nulls = pd.DataFrame(train_data)[continuous_cols].isna().any().any()\n",
    "\n",
    "        if any_nulls:\n",
    "            raise ValueError(\n",
    "                'CTGAN does not support null values in the continuous training data. '\n",
    "                'Please remove all null values from your continuous training data.'\n",
    "            )\n",
    "    def fit(self, train_data, discrete_columns=(), epochs=300, batch_size=500, track = False):\n",
    "        assert batch_size % 2 == 0\n",
    "        self._validate_discrete_columns(train_data, discrete_columns)\n",
    "        self._validate_null_data(train_data, discrete_columns)\n",
    "\n",
    "        self._transformer = DataTransformer()\n",
    "        self._transformer.fit(train_data, discrete_columns)\n",
    "\n",
    "        train_data = self._transformer.transform(train_data)\n",
    "\n",
    "        self._data_sampler = DataSampler(\n",
    "            train_data, self._transformer.output_info_list, self._log_frequency\n",
    "        )\n",
    "\n",
    "        data_dim = self._transformer.output_dimensions\n",
    "\n",
    "        self._generator = Generator(\n",
    "            self._embedding_dim + self._data_sampler.dim_cond_vec(), self._generator_dim, data_dim\n",
    "        ).to(self._device)\n",
    "\n",
    "        discriminator = Discriminator(\n",
    "            data_dim + self._data_sampler.dim_cond_vec(), self._discriminator_dim, pac=self.pac\n",
    "        ).to(self._device)\n",
    "\n",
    "        optimizerG = optim.Adam(\n",
    "            self._generator.parameters(),\n",
    "            lr=self._generator_lr,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=self._generator_decay,\n",
    "        )\n",
    "\n",
    "        optimizerD = optim.Adam(\n",
    "            discriminator.parameters(),\n",
    "            lr=self._discriminator_lr,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=self._discriminator_decay,\n",
    "        )\n",
    "\n",
    "        mean = torch.zeros(batch_size, self._embedding_dim, device=self._device)\n",
    "        std = mean + 1\n",
    "\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Distriminator Loss'])\n",
    "\n",
    "\n",
    "        steps_per_epoch = max(len(train_data) // batch_size, 1)\n",
    "        if track:\n",
    "            run = wandb.init(project=\"Data Augmentation - CTGAN\", name=\"ctgan\", config={\n",
    "                \"epochs\": epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"generator_lr\": self._generator_lr,\n",
    "                \"discriminator_lr\": self._discriminator_lr,\n",
    "                \"pac\": self.pac,\n",
    "                \"data_dim\": data_dim\n",
    "            })\n",
    "        for i in range(epochs):\n",
    "            for id_ in range(steps_per_epoch):\n",
    "                for n in range(self._discriminator_steps):\n",
    "                    fakez = torch.normal(mean=mean, std=std)\n",
    "\n",
    "                    condvec = self._data_sampler.sample_condvec(batch_size)\n",
    "                    if condvec is None:\n",
    "                        c1, m1, col, opt = None, None, None, None\n",
    "                        real = self._data_sampler.sample_data(\n",
    "                            train_data, batch_size, col, opt\n",
    "                        )\n",
    "                    else:\n",
    "                        c1, m1, col, opt = condvec\n",
    "                        c1 = torch.from_numpy(c1).to(self._device)\n",
    "                        m1 = torch.from_numpy(m1).to(self._device)\n",
    "                        fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                        perm = np.arange(batch_size)\n",
    "                        np.random.shuffle(perm)\n",
    "                        real = self._data_sampler.sample_data(\n",
    "                            train_data, batch_size, col[perm], opt[perm]\n",
    "                        )\n",
    "                        c2 = c1[perm]\n",
    "\n",
    "                    fake = self._generator(fakez)\n",
    "                    fakeact = self._apply_activate(fake)\n",
    "\n",
    "                    real = torch.from_numpy(real.astype('float32')).to(self._device)\n",
    "\n",
    "                    if c1 is not None:\n",
    "                        fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                        real_cat = torch.cat([real, c2], dim=1)\n",
    "                    else:\n",
    "                        real_cat = real\n",
    "                        fake_cat = fakeact\n",
    "\n",
    "                    y_fake = discriminator(fake_cat)\n",
    "                    y_real = discriminator(real_cat)\n",
    "\n",
    "                    pen = discriminator.calc_gradient_penalty(\n",
    "                        real_cat, fake_cat, self._device, self.pac\n",
    "                    )\n",
    "                    loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "                    optimizerD.zero_grad(set_to_none=False)\n",
    "                    pen.backward(retain_graph=True)\n",
    "                    loss_d.backward()\n",
    "                    optimizerD.step()\n",
    "\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "                condvec = self._data_sampler.sample_condvec(batch_size)\n",
    "\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self._device)\n",
    "                    m1 = torch.from_numpy(m1).to(self._device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                fake = self._generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "                else:\n",
    "                    y_fake = discriminator(fakeact)\n",
    "\n",
    "                if condvec is None:\n",
    "                    cross_entropy = 0\n",
    "                else:\n",
    "                    cross_entropy = self._cond_loss(fake, c1, m1)\n",
    "\n",
    "                loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "                optimizerG.zero_grad(set_to_none=False)\n",
    "                loss_g.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "            generator_loss = loss_g.detach().cpu().item()\n",
    "            discriminator_loss = loss_d.detach().cpu().item()\n",
    "            if track:\n",
    "                wandb.log({\n",
    "                \"epoch\": i,\n",
    "                \"generator_loss\": generator_loss,\n",
    "                \"discriminator_loss\": discriminator_loss\n",
    "                })\n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                print(f\"Epoch [{i+1}/{epochs}], \"\n",
    "                      f\"Train - G Loss: {generator_loss:.4f}, D Loss: {discriminator_loss:.4f}, \"\n",
    "                      f\"LR - Generator: {self._generator_lr:.6f}, Discriminator: {self._discriminator_lr:.6f}\")\n",
    "            epoch_loss_df = pd.DataFrame({\n",
    "                'Epoch': [i],\n",
    "                'Generator Loss': [generator_loss],\n",
    "                'Discriminator Loss': [discriminator_loss],\n",
    "            })\n",
    "            if not self.loss_values.empty:\n",
    "                self.loss_values = pd.concat([self.loss_values, epoch_loss_df]).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "            else:\n",
    "                self.loss_values = epoch_loss_df\n",
    "        wandb.finish()\n",
    "        \n",
    "    def sample(self, n, condition_column=None, condition_value=None, batch_size= 500):\n",
    "        if condition_column is not None and condition_value is not None:\n",
    "            condition_info = self._transformer.convert_column_name_value_to_id(\n",
    "                condition_column, condition_value\n",
    "            )\n",
    "            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n",
    "                condition_info, batch_size\n",
    "            )\n",
    "        else:\n",
    "            global_condition_vec = None\n",
    "\n",
    "        steps = n // batch_size + 1\n",
    "        data = []\n",
    "        for i in range(steps):\n",
    "            mean = torch.zeros(batch_size, self._embedding_dim)\n",
    "            std = mean + 1\n",
    "            fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
    "\n",
    "            if global_condition_vec is not None:\n",
    "                condvec = global_condition_vec.copy()\n",
    "            else:\n",
    "                condvec = self._data_sampler.sample_original_condvec(batch_size)\n",
    "\n",
    "            if condvec is None:\n",
    "                pass\n",
    "            else:\n",
    "                c1 = condvec\n",
    "                c1 = torch.from_numpy(c1).to(self._device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            fake = self._generator(fakez)\n",
    "            fakeact = self._apply_activate(fake)\n",
    "            data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:n]\n",
    "\n",
    "        return self._transformer.inverse_transform(data)\n",
    "\n",
    "    def set_device(self, device):\n",
    "        self._device = device\n",
    "        if self._generator is not None:\n",
    "            self._generator.to(self._device)\n",
    "    def save(self, path):\n",
    "        if self._generator is None or self._discriminator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        checkpoint = {\n",
    "            'generator': self._generator.state_dict(),\n",
    "            'discriminator': self._discriminator.state_dict(),\n",
    "            'generator_optimizer': self.optimizerG.state_dict(),\n",
    "            'discriminator_optimizer': self.optimizerD.state_dict(),\n",
    "            'transformer': self._transformer  \n",
    "        }\n",
    "        torch.save(checkpoint, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d840576a-4765-49ca-82bd-e85e95dca37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train - G Loss: 0.7460, D Loss: 0.0235, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [10/100], Train - G Loss: 0.6551, D Loss: 0.0164, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [20/100], Train - G Loss: 0.7027, D Loss: -0.0121, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [30/100], Train - G Loss: 0.7212, D Loss: 0.0645, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [40/100], Train - G Loss: 0.4899, D Loss: 0.0557, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [50/100], Train - G Loss: 0.4857, D Loss: 0.0158, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [60/100], Train - G Loss: 0.5198, D Loss: 0.0043, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [70/100], Train - G Loss: 0.4258, D Loss: 0.0398, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [80/100], Train - G Loss: 0.3130, D Loss: -0.0181, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [90/100], Train - G Loss: 0.3534, D Loss: 0.1030, LR - Generator: 0.000200, Discriminator: 0.000200\n",
      "Epoch [100/100], Train - G Loss: 0.3269, D Loss: -0.1113, LR - Generator: 0.000200, Discriminator: 0.000200\n"
     ]
    }
   ],
   "source": [
    "a = pd.DataFrame({\n",
    "    'A': [1,2,1,2,1,2,1,3,3,2,2,2,1,1,1], \n",
    "    'B': ['z','z','zz', 'z', 'z','z','z','zz', 'z', 'z','z','z','zz', 'z', 'z'], \n",
    "    'C': [0.99404096, 0.58273721, 0.21701061, 0.1175965,  0.68291119, 0.62865904, \n",
    "          0.68754258, 0.51539969, 0.70036077, 0.94512348, 0.13780938, 0.04576671,\n",
    "          0.0784216, 0.19138225, 0.78545446]\n",
    "})\n",
    "\n",
    "# Create a copy of the dataframe to preserve original\n",
    "df = a.copy()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = ['A', 'B']\n",
    "numeric_columns = ['C']\n",
    "ctgan = CTGAN()\n",
    "ctgan.fit(a, categorical_columns,epochs= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0e11886-5809-48e2-9e98-d9d17bee3f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>z</td>\n",
       "      <td>0.991171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>z</td>\n",
       "      <td>1.560485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>zz</td>\n",
       "      <td>0.044675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>zz</td>\n",
       "      <td>0.947360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>z</td>\n",
       "      <td>1.321150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>z</td>\n",
       "      <td>0.526378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>zz</td>\n",
       "      <td>0.112976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>z</td>\n",
       "      <td>0.530903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>z</td>\n",
       "      <td>1.207839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>z</td>\n",
       "      <td>1.546595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>z</td>\n",
       "      <td>0.251251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>z</td>\n",
       "      <td>0.819802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>z</td>\n",
       "      <td>1.275956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>z</td>\n",
       "      <td>1.171638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>zz</td>\n",
       "      <td>1.393879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>z</td>\n",
       "      <td>1.451684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A   B         C\n",
       "0   2   z  0.991171\n",
       "1   2   z  1.560485\n",
       "2   2  zz  0.044675\n",
       "3   3  zz  0.947360\n",
       "4   1   z  1.321150\n",
       "5   3   z  0.526378\n",
       "6   1  zz  0.112976\n",
       "7   1   z  0.530903\n",
       "8   3   z  1.207839\n",
       "9   2   z  1.546595\n",
       "10  3   z  0.251251\n",
       "11  3   z  0.819802\n",
       "12  2   z  1.275956\n",
       "13  2   z  1.171638\n",
       "14  1  zz  1.393879\n",
       "15  3   z  1.451684"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctgan.sample(n=16, condition_column='A', condition_value= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc80d631-f969-48e2-8d4b-a64b5b04bd97",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CTGAN' object has no attribute '_discriminator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ctgan\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 382\u001b[0m, in \u001b[0;36mCTGAN.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discriminator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel not trained. Call fit() first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    384\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discriminator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer  \n\u001b[0;32m    390\u001b[0m     }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CTGAN' object has no attribute '_discriminator'"
     ]
    }
   ],
   "source": [
    "ctgan.save(\"a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Environment",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
