{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa7948e-e64e-4785-b540-4fa8c44b6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212d67bf-46da-42b1-ad08-cad3d4551dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruits_banane</th>\n",
       "      <th>fruits_orange</th>\n",
       "      <th>fruits_pomme</th>\n",
       "      <th>fruits_cat_pomme</th>\n",
       "      <th>fruits_cat_banane</th>\n",
       "      <th>fruits_cat_orange</th>\n",
       "      <th>fruits_cat_fraise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruits_banane  fruits_orange  fruits_pomme  fruits_cat_pomme  \\\n",
       "0          False          False          True              True   \n",
       "1           True          False         False             False   \n",
       "2          False          False          True              True   \n",
       "3          False           True         False             False   \n",
       "4           True          False         False             False   \n",
       "\n",
       "   fruits_cat_banane  fruits_cat_orange  fruits_cat_fraise  \n",
       "0              False              False              False  \n",
       "1               True              False              False  \n",
       "2              False              False              False  \n",
       "3              False               True              False  \n",
       "4               True              False              False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'fruits': ['pomme', 'banane', 'pomme', 'orange', 'banane']\n",
    "})\n",
    "\n",
    "# Catégoriser la colonne 'fruits' avec des catégories prédéfinies\n",
    "data['fruits_cat'] = pd.Categorical(data['fruits'], categories=['pomme', 'banane', 'orange', 'fraise'])\n",
    "pd.get_dummies(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "87017da0-6b5d-487a-b872-7947616d7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondVec:\n",
    "    def __init__(self, data, categorical_columns, categorical_dims):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.n_categories = sum(categorical_dims.values())\n",
    "        self.n_features = len(categorical_columns)\n",
    "        self.data = data\n",
    "        \n",
    "    def sample_conditional_vector(self, batch_size):\n",
    "        \"\"\"Sample conditional vectors for training.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None, None\n",
    "        \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        mask = np.zeros((batch_size, self.n_features), dtype='float32')\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Choose a random discrete column\n",
    "            feature_idx = np.random.choice(range(self.n_features))\n",
    "            feature = self.categorical_columns[feature_idx]\n",
    "            \n",
    "            # Choose a random category from that column\n",
    "            feature_dim = self.categorical_dims[feature]\n",
    "            category_idx = np.random.choice(range(feature_dim))\n",
    "            \n",
    "            # Set mask and vec values\n",
    "            mask[i, feature_idx] = 1\n",
    "            vec[i, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "            \n",
    "        return torch.from_numpy(vec), torch.from_numpy(mask)\n",
    "    \n",
    "    def generate_conditional_vector(self, conditions, batch_size):\n",
    "        \"\"\"Generate conditional vector based on conditions.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None\n",
    "            \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        for feature, category in conditions.items():\n",
    "            if feature in self.categorical_columns:\n",
    "                feature_idx = self.categorical_columns.index(feature)\n",
    "                category_idx = int(category)  # Assuming category is an index\n",
    "                \n",
    "                vec[:, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "        \n",
    "        return torch.from_numpy(vec)\n",
    "    \n",
    "class CTGANDataset(Dataset):\n",
    "    def __init__(self, data, categorical_columns=None):\n",
    "        self.data = data\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.continuous_columns = [col for col in data.columns if col not in self.categorical_columns]\n",
    "        \n",
    "        # Create encoders for categorical columns and fit GMMs for continuous columns\n",
    "        self.cond_vec = None\n",
    "        self.transformer = DataTransformer(self.categorical_columns)\n",
    "        self.transformer.fit(data)\n",
    "        self.transformed_data = self.transformer.transform(data)\n",
    "        \n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.cond_vec = CondVec(\n",
    "                data, \n",
    "                categorical_columns=self.categorical_columns,\n",
    "                categorical_dims=self.transformer.categorical_dims\n",
    "            )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transformed_data[idx]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample data and conditional vectors for training.\"\"\"\n",
    "        # Sample data\n",
    "        idx = np.random.choice(range(len(self)), batch_size)\n",
    "        data = self.transformed_data[idx]\n",
    "        \n",
    "        # Sample conditional vectors if categorical columns exist\n",
    "        if self.cond_vec:\n",
    "            cond_vec, mask = self.cond_vec.sample_conditional_vector(batch_size)\n",
    "            return data, cond_vec, mask\n",
    "        \n",
    "        return data, None, None\n",
    "        \n",
    "class DataTransformer:\n",
    "    \"\"\"Transforms data between original space and CTGAN transformed space.\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_columns):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.categorical_dims = {}\n",
    "        self.continuous_gmms = {}\n",
    "        self.n_clusters = 10  # Number of modes for GMM\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the data transformer.\"\"\"\n",
    "        # Process categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            categories = pd.Categorical(data[column]).categories\n",
    "            self.categorical_dims[column] = len(categories)\n",
    "            \n",
    "        # Process continuous columns by fitting GMMs\n",
    "        continuous_columns = [c for c in data.columns if c not in self.categorical_columns]\n",
    "        for column in continuous_columns:\n",
    "            col_data = data[column].values.reshape(-1, 1)\n",
    "            gmm = GaussianMixture(n_components=self.n_clusters)\n",
    "            gmm.fit(col_data)\n",
    "            self.continuous_gmms[column] = gmm\n",
    "            \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform data to CTGAN format.\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        # Transform categorical columns to one-hot encoding\n",
    "        for column in self.categorical_columns:\n",
    "            one_hot = pd.get_dummies(data[column], prefix=column)\n",
    "            result.append(one_hot.values)\n",
    "            \n",
    "        # Transform continuous columns with mode-specific normalization\n",
    "        for column in data.columns:\n",
    "            if column not in self.categorical_columns:\n",
    "                col_data = data[column].values.reshape(-1, 1)\n",
    "                gmm = self.continuous_gmms[column]\n",
    "                \n",
    "                # Get cluster assignments and probabilities\n",
    "                clusters = gmm.predict(col_data)\n",
    "                probs = gmm.predict_proba(col_data)\n",
    "                \n",
    "                # Normalize data based on Gaussian parameters\n",
    "                normalized = np.zeros_like(col_data)\n",
    "                for i in range(len(col_data)):\n",
    "                    cluster = clusters[i]\n",
    "                    mean = gmm.means_[cluster][0]\n",
    "                    std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                    normalized[i] = (col_data[i] - mean) / (4 * std)\n",
    "                \n",
    "                # Create encoded data: [normalized value, cluster_1_prob, ..., cluster_k_prob]\n",
    "                encoded = np.zeros((len(col_data), self.n_clusters + 1))\n",
    "                encoded[:, 0] = normalized.flatten()\n",
    "                encoded[:, 1:] = probs\n",
    "                \n",
    "                result.append(encoded)\n",
    "                \n",
    "        # Combine all transformed columns\n",
    "        if result:\n",
    "            return np.concatenate(result, axis=1).astype('float32')\n",
    "        return np.zeros((len(data), 0))\n",
    "        \n",
    "    def inverse_transform(self, transformed_data):\n",
    "        \"\"\"Convert transformed data back to original format.\"\"\"\n",
    "        # Create a DataFrame for the result\n",
    "        result = pd.DataFrame()\n",
    "        column_idx = 0\n",
    "        \n",
    "        # Inverse transform categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            dim = self.categorical_dims[column]\n",
    "            one_hot = transformed_data[:, column_idx:column_idx + dim]\n",
    "            \n",
    "            # Convert one-hot back to categorical\n",
    "            indices = np.argmax(one_hot, axis=1)\n",
    "            # Récupérer les catégories originales\n",
    "            try:\n",
    "                categories = pd.Categorical(self.data[column]).categories\n",
    "                result[column] = pd.Categorical.from_codes(indices, categories=categories)\n",
    "            except:\n",
    "                # Fallback en cas d'erreur\n",
    "                result[column] = indices\n",
    "            \n",
    "            column_idx += dim\n",
    "            \n",
    "        # Inverse transform continuous columns\n",
    "        for column in self.continuous_gmms:\n",
    "            gmm = self.continuous_gmms[column]\n",
    "            \n",
    "            # Extract normalized value and cluster probabilities\n",
    "            normalized = transformed_data[:, column_idx]\n",
    "            probs = transformed_data[:, column_idx + 1:column_idx + 1 + self.n_clusters]\n",
    "            \n",
    "            # Convert back to original space\n",
    "            cluster_idx = np.argmax(probs, axis=1)\n",
    "            values = np.zeros(len(normalized))\n",
    "            \n",
    "            for i in range(len(normalized)):\n",
    "                cluster = cluster_idx[i]\n",
    "                mean = gmm.means_[cluster][0]\n",
    "                std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                values[i] = normalized[i] * (4 * std) + mean\n",
    "                \n",
    "            result[column] = values\n",
    "            column_idx += self.n_clusters + 1\n",
    "            \n",
    "        return result\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_categories=0, hidden_dims=[256, 256]):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        dims = [input_dim + n_categories] + hidden_dims + [output_dim]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:\n",
    "                self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "                self.layers.append(nn.ReLU())\n",
    "        \n",
    "    def forward(self, noise, cond_vec=None):\n",
    "        if cond_vec is not None:\n",
    "            x = torch.cat([noise, cond_vec], dim=1)\n",
    "\n",
    "        else:\n",
    "            x = noise\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, n_categories=0, hidden_dims=[256, 128]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.pac = 1  # Default no pac\n",
    "        \n",
    "        # Placeholder for main layers - will be initialized in set_pac\n",
    "        self.main_layers = None\n",
    "        self.output_layer = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Store parameters for layer initialization\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_categories = n_categories\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Initialize layers with default pac=1\n",
    "        self._init_layers()\n",
    "            \n",
    "    def _init_layers(self):\n",
    "        \"\"\"Initialize network layers based on current pac value\"\"\"\n",
    "        pac_input_dim = self.input_dim * self.pac\n",
    "        \n",
    "        self.main_layers = nn.ModuleList()\n",
    "        for i in range(len(self.hidden_dims)):\n",
    "            if i == 0:\n",
    "                self.main_layers.append(nn.Linear(pac_input_dim, self.hidden_dims[i]))\n",
    "            else:\n",
    "                self.main_layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
    "            self.main_layers.append(nn.LeakyReLU(0.2))\n",
    "        self.main_layers.to(self.device)\n",
    "        self.output_layer = nn.Linear(self.hidden_dims[-1], 1, device= self.device)\n",
    "        \n",
    "        # Conditional embedding layers\n",
    "        self.cond_layers = None\n",
    "        if self.n_categories > 0:\n",
    "            \n",
    "            self.cond_layers = nn.Sequential(\n",
    "                nn.Linear(self.n_categories, pac_input_dim),\n",
    "                nn.ReLU()\n",
    "            ).to(self.device)\n",
    "    \n",
    "    def set_pac(self, pac):\n",
    "        \"\"\"Update the model to handle pac-sized inputs\"\"\"\n",
    "        self.pac = pac\n",
    "        self._init_layers()\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        if cond_vec is not None and self.cond_layers is not None:\n",
    "            cond = self.cond_layers(cond_vec)\n",
    "            x = x + cond\n",
    "            \n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class PacGan(nn.Module):\n",
    "    \"\"\"PacGAN discriminator for improved GAN training stability.\"\"\"\n",
    "    def __init__(self, discriminator, pac=10):\n",
    "        super(PacGan, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.pac = pac\n",
    "        # Tell the discriminator about pac to handle dimensions\n",
    "        if hasattr(self.discriminator, 'set_pac'):\n",
    "            self.discriminator.set_pac(pac)\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        batch_size = x.size(0)\n",
    "        if batch_size % self.pac != 0:\n",
    "            # Padding to make divisible by pac\n",
    "            pad_size = self.pac - (batch_size % self.pac)\n",
    "            indices = np.random.choice(batch_size, pad_size)\n",
    "            x = torch.cat([x, x[indices]], dim=0)\n",
    "            if cond_vec is not None:\n",
    "                cond_vec = torch.cat([cond_vec, cond_vec[indices]], dim=0)\n",
    "                \n",
    "        # Reshape x for PacGAN structure\n",
    "        new_batch_size = x.size(0) // self.pac\n",
    "        x_reshaped = x.view(new_batch_size, self.pac * x.size(1))\n",
    "        \n",
    "        # For conditional vectors, we need to have one per batch\n",
    "        if cond_vec is not None:\n",
    "            # Take one conditional vector per pac group\n",
    "            cond_vec_reshaped = cond_vec.view(new_batch_size, self.pac, cond_vec.size(1))\n",
    "            cond_vec_flat = cond_vec_reshaped[:, 0, :]  # Just take the first one\n",
    "            return self.discriminator(x_reshaped, cond_vec_flat)\n",
    "        else:\n",
    "            return self.discriminator(x_reshaped, None)\n",
    "\n",
    "class CTGAN:\n",
    "    def __init__(self, categorical_columns=None, noise_dim=100, batch_size=500, \n",
    "                 generator_lr=2e-4, discriminator_lr=2e-4, pac=10):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.noise_dim = noise_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.generator_lr = generator_lr\n",
    "        self.discriminator_lr = discriminator_lr\n",
    "        self.pac = pac\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.transformer = None\n",
    "        self.dataset = None\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        \n",
    "    def fit(self, data, epochs=300):\n",
    "        \"\"\"Fit CTGAN to the data\"\"\"\n",
    "        # Create dataset\n",
    "        self.dataset = CTGANDataset(data, categorical_columns=self.categorical_columns)\n",
    "        self.transformer = self.dataset.transformer\n",
    "        \n",
    "        # Calculate dimensions\n",
    "        data_dim = self.dataset.transformed_data.shape[1]\n",
    "        n_categories = 0\n",
    "        if self.dataset.cond_vec:\n",
    "            n_categories = self.dataset.cond_vec.n_categories\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim, \n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        generator_optimizer = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=self.generator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        discriminator_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=self.discriminator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Ensure batch size is a multiple of pac for PacGAN\n",
    "        batch_size = (self.batch_size // self.pac) * self.pac\n",
    "        if batch_size == 0:\n",
    "            batch_size = self.pac\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            for _ in range(max(1, len(self.dataset) // batch_size)):\n",
    "                # Get real data and conditional vectors\n",
    "                real_data, cond_vec, mask = self.dataset.sample(batch_size)\n",
    "                real_data = torch.from_numpy(real_data).to(self.device)\n",
    "                \n",
    "                if cond_vec is not None:\n",
    "                    cond_vec = cond_vec.to(self.device)\n",
    "                    mask = mask.to(self.device)\n",
    "                \n",
    "                # Labels for real and fake data\n",
    "                real_labels = torch.ones(batch_size // self.pac, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size // self.pac, 1).to(self.device)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                discriminator_optimizer.zero_grad()\n",
    "                \n",
    "                # Real data loss\n",
    "                outputs = self.discriminator(real_data, cond_vec)\n",
    "                d_real_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                # Generate fake data\n",
    "                noise = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
    "                fake_data = self.generator(noise, cond_vec)\n",
    "                \n",
    "                outputs = self.discriminator(fake_data.detach(), cond_vec)\n",
    "                d_fake_loss = criterion(outputs, fake_labels)\n",
    "                \n",
    "                d_loss = d_real_loss + d_fake_loss\n",
    "                d_loss.backward()\n",
    "                discriminator_optimizer.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                generator_optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.discriminator(fake_data, cond_vec)\n",
    "                g_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], G Loss: {np.mean(g_losses):.4f}, D Loss: {np.mean(d_losses):.4f}, Learning rate\")\n",
    "    \n",
    "    def generate(self, n_samples, conditions= None):\n",
    "        \"\"\"Generate synthetic samples with optional conditioning.\"\"\"\n",
    "        if self.generator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        self.generator.eval()\n",
    "        \n",
    "        steps = n_samples // self.batch_size + 1\n",
    "        data = []\n",
    "        \n",
    "        for i in range(steps):\n",
    "            n_batch = min(self.batch_size, n_samples - i * self.batch_size)\n",
    "            if n_batch <= 0:\n",
    "                break\n",
    "                \n",
    "            # Generate noise\n",
    "            noise = torch.randn(n_batch, self.noise_dim).to(self.device)\n",
    "            \n",
    "            # Generate conditional vector if necessary\n",
    "            cond_vec = None\n",
    "            if self.dataset.cond_vec and conditions:\n",
    "                print(n_batch)\n",
    "                cond_vec = self.dataset.cond_vec.generate_conditional_vector(conditions, n_batch)\n",
    "                cond_vec = cond_vec.to(self.device)\n",
    "                \n",
    "            # Generate data\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                fake = self.generator(noise, cond_vec)\n",
    "            data.append(fake.cpu().numpy())\n",
    "            \n",
    "        data = np.concatenate(data, axis=0)\n",
    "        \n",
    "        # Convert to the original data format\n",
    "        synthetic_data = self.transformer.inverse_transform(data[:n_samples])\n",
    "        print(synthetic_data.shape)\n",
    "        return synthetic_data\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        if self.generator is None or self.discriminator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        state = {\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'transformer': self.transformer\n",
    "        }\n",
    "        \n",
    "        torch.save(state, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        state = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.noise_dim = state['noise_dim']\n",
    "        self.categorical_columns = state['categorical_columns']\n",
    "        self.transformer = state['transformer']\n",
    "        \n",
    "        # Recreate the dataset and models\n",
    "        n_categories = 0\n",
    "        data_dim = 0\n",
    "        \n",
    "        if hasattr(self.transformer, 'categorical_dims'):\n",
    "            n_categories = sum(self.transformer.categorical_dims.values())\n",
    "            if hasattr(self.transformer, 'continuous_gmms'):\n",
    "                continuous_dims = sum([gmm.n_components + 1 for gmm in self.transformer.continuous_gmms.values()])\n",
    "                data_dim = n_categories + continuous_dims\n",
    "        \n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim,\n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        self.generator.load_state_dict(state['generator'])\n",
    "        self.discriminator.load_state_dict(state['discriminator'])\n",
    "        \n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc59e6-70a4-4b52-9af6-beef7f2bfae3",
   "metadata": {},
   "source": [
    "# Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "db188c46-467a-4aaa-a35a-1690d508e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n",
      "100\n",
      "(100, 10)\n",
      "\n",
      "Real data statistics:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   MedHouseVal  HouseAge_Cat  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558      2.196560  \n",
      "std       10.386050      2.135952      2.003532      1.153956      1.229386  \n",
      "min        0.692308     32.540000   -124.350000      0.149990      0.000000  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000      1.000000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000      2.000000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250      3.000000  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010      4.000000  \n",
      "\n",
      "Conditioned data (HouseAge_Cat = 2) statistics:\n",
      "       HouseAge_Cat      MedInc    HouseAge    AveRooms   AveBedrms  \\\n",
      "count    100.000000  100.000000  100.000000  100.000000  100.000000   \n",
      "mean       1.550000    4.625573   26.709094   21.762026    5.754796   \n",
      "std        1.242147    2.353406   17.028676   37.212154    9.739052   \n",
      "min        0.000000    1.810874   -0.361337    3.009988    0.542161   \n",
      "25%        0.000000    2.822722    9.712743    5.075253    0.973770   \n",
      "50%        2.000000    4.230142   28.601992    5.842839    1.053742   \n",
      "75%        2.000000    5.390607   43.038362   14.304566    1.856286   \n",
      "max        4.000000   13.643610   52.158819  152.466583   34.067144   \n",
      "\n",
      "        Population     AveOccup    Latitude   Longitude  MedHouseVal  \n",
      "count   100.000000   100.000000  100.000000  100.000000   100.000000  \n",
      "mean   1597.433421    52.192416   34.612134 -119.392429     2.063663  \n",
      "std    1353.116868   152.721957    1.948065    1.687568     1.317290  \n",
      "min     418.617636     0.376895   32.364012 -122.777103     0.273901  \n",
      "25%     891.282472     2.007350   33.865567 -120.745481     1.030614  \n",
      "50%    1117.404337     2.863568   34.058180 -119.110835     1.813429  \n",
      "75%    1502.279226     6.790517   34.287797 -118.222550     2.523651  \n",
      "max    7261.359226  1243.334965   42.734143 -115.900043     5.000680  \n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.DataTransformer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataTransformer])` or the `torch.serialization.safe_globals([DataTransformer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m     example_usage()\n",
      "Cell \u001b[1;32mIn[156], line 46\u001b[0m, in \u001b[0;36mexample_usage\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m     45\u001b[0m new_ctgan \u001b[38;5;241m=\u001b[39m CTGAN()\n\u001b[1;32m---> 46\u001b[0m new_ctgan\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctgan_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n",
      "Cell \u001b[1;32mIn[154], line 477\u001b[0m, in \u001b[0;36mCTGAN.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m    476\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the model.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_dim \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_columns\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.DataTransformer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataTransformer])` or the `torch.serialization.safe_globals([DataTransformer])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def example_usage():\n",
    "    # Sample tabular data\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    data = fetch_california_housing(as_frame=True).frame\n",
    "    \n",
    "    # For this example, let's convert HouseAge to categorical by binning\n",
    "    data['HouseAge_Cat'] = pd.cut(data['HouseAge'], bins=5, labels=False)\n",
    "    categorical_columns = ['HouseAge_Cat']\n",
    "    # Initialize CTGAN\n",
    "    ctgan = CTGAN(categorical_columns=categorical_columns, pac=5)  # Ajusté le pac à 5 pour éviter des problèmes de dimensionnalité\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"Training CTGAN model...\")\n",
    "    ctgan.fit(data, epochs=1)\n",
    "    \n",
    "    # Generate synthetic data (unconditional)\n",
    "    #print(\"Generating synthetic data...\")\n",
    "    #synthetic_data = ctgan.generate(n_samples=1000)\n",
    "    \n",
    "    # Generate synthetic data with conditions\n",
    "    # Example condition: HouseAge_Cat = 2\"\"\"\n",
    "    conditioned_data = ctgan.generate(\n",
    "        n_samples=100,\n",
    "        conditions={'HouseAge_Cat': 2}\n",
    "    )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"\\nReal data statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    #print(\"\\nSynthetic data statistics:\")\n",
    "    #print(synthetic_data.describe())\n",
    "    \n",
    "    print(\"\\nConditioned data (HouseAge_Cat = 2) statistics:\")\n",
    "    print(conditioned_data.describe())\n",
    "    \n",
    "    # Save model\n",
    "    ctgan.save(\"ctgan_model.pt\")\n",
    "    \n",
    "    # Load model\n",
    "    new_ctgan = CTGAN()\n",
    "    new_ctgan.load(\"ctgan_model.pt\")\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77bd8342-b732-4dc3-9a9a-8707086462a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_california_housing(as_frame=True).frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06960d-3217-43c2-af56-d154d5b0c5d6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8909627b-2bb3-461b-ac60-baa361fa3b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n",
      "errr\n",
      "['b']\n",
      "{'b': 2}\n",
      "errr\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2])\n",
      "torch.Size([5, 1])\n",
      "torch.Size([5, 2])\n",
      "Generating synthetic data...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 3x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     example_usage()\n",
      "Cell \u001b[1;32mIn[90], line 21\u001b[0m, in \u001b[0;36mexample_usage\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Generate synthetic data (unconditional)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating synthetic data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39mgenerate(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Generate synthetic data with conditions\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Example condition: HouseAge_Cat = 2\u001b[39;00m\n\u001b[0;32m     25\u001b[0m conditioned_data \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39mgenerate( n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, conditions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m} )\n",
      "Cell \u001b[1;32mIn[88], line 456\u001b[0m, in \u001b[0;36mCTGAN.generate\u001b[1;34m(self, n_samples, conditions)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# Generate data\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 456\u001b[0m         fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(noise, cond_vec)\n\u001b[0;32m    457\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(fake\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    459\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[88], line 222\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, noise, cond_vec)\u001b[0m\n\u001b[0;32m    219\u001b[0m     x \u001b[38;5;241m=\u001b[39m noise\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 222\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 3x256)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def example_usage():\n",
    "    # Sample tabular data\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.DataFrame({'a': [1,2,3,4,5,6,7,8,9,10], 'b': [1,-1,-1,1,1,1,1,1,1,-1 ]})\n",
    "    data['b'] = pd.Categorical(data['b'])\n",
    "    pd.Categorical(data['b']).categories\n",
    "    categorical_columns = ['b']\n",
    "    # Initialize CTGAN\n",
    "    ctgan = CTGAN(categorical_columns=categorical_columns, pac=5, noise_dim=1, batch_size=1)  # Ajusté le pac à 5 pour éviter des problèmes de dimensionnalité\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"Training CTGAN model...\")\n",
    "    ctgan.fit(data, epochs=1)\n",
    "    \n",
    "    # Generate synthetic data (unconditional)\n",
    "    print(\"Generating synthetic data...\")\n",
    "    synthetic_data = ctgan.generate(n_samples=1000)\n",
    "    \n",
    "    # Generate synthetic data with conditions\n",
    "    # Example condition: HouseAge_Cat = 2\n",
    "    conditioned_data = ctgan.generate( n_samples=1000, conditions = {'b': 1} )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"\\nReal data statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    print(\"\\nSynthetic data statistics:\")\n",
    "    print(synthetic_data.describe())\n",
    "    \n",
    "    print(\"\\nConditioned data (HouseAge_Cat = 2) statistics:\")\n",
    "    print(conditioned_data.describe())\n",
    "    \n",
    "    # Save model\n",
    "    ctgan.save(\"ctgan_model.pt\")\n",
    "    \n",
    "    # Load model\n",
    "    new_ctgan = CTGAN()\n",
    "    new_ctgan.load(\"ctgan_model.pt\")\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b856ea39-d2a0-4694-9116-7eb2d81ccb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CTGANDataset(data, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e2f4ca-5ba7-4ee1-8ad9-e6d22a192685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['z', 'zz', 'zzz'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame({'a': [1,2,3,4,5,6,7,8,9,10], 'b': ['z','zz','zz','z','zzz','z','zzz','zzz','z','zzz' ]})\n",
    "a['b'] = pd.Categorical(a['b'])\n",
    "pd.Categorical(a['b']).categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e34cc4e-facf-4542-919b-6f98c3704c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32),\n",
       " tensor([[0., 1., 0.]]),\n",
       " tensor([[1.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = CTGANDataset(a, categorical_columns= ['b'])\n",
    "aa.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79b3a27c-2c2a-4775-b4d4-e5862138b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.1629847e-11,\n",
       "        9.9996722e-01, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 3.2581773e-02,\n",
       "        6.3743941e-02, 1.0747959e-42],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 6.6141941e-02,\n",
       "        4.1500479e-02, 3.8707367e-40],\n",
       "       ...,\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 8.7430551e-26,\n",
       "        2.6141222e-21, 5.6873501e-04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 1.5613448e-27,\n",
       "        3.4088894e-22, 6.9994559e-05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 1.9312942e-26,\n",
       "        1.2193060e-21, 2.6615727e-04]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c15ca-9c83-47b8-aa58-dd49b8818871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = self.dataset.transformed_data.shape[1]\n",
    "        n_categories = 0\n",
    "        if self.dataset.cond_vec:\n",
    "            n_categories = self.dataset.cond_vec.n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8435335-41a7-412e-8574-cae5329aa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTransformer(['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ad093747-dff0-4def-b592-80d75bf2a6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dt\u001b[38;5;241m.\u001b[39mtransform(a)\n",
      "Cell \u001b[1;32mIn[49], line 122\u001b[0m, in \u001b[0;36mDataTransformer.transform\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns:\n\u001b[0;32m    121\u001b[0m     col_data \u001b[38;5;241m=\u001b[39m data[column]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m     gmm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_gmms[column]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# Get cluster assignments and probabilities\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(col_data)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "dt.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6c7a4-e302-47da-92ce-8dd9dd32e4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Environment",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
