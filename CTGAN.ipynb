{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa7948e-e64e-4785-b540-4fa8c44b6027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n",
      "Epoch [1/300], G Loss: 0.7274, D Loss: 1.4008\n",
      "Epoch [50/300], G Loss: 0.8291, D Loss: 1.2470\n",
      "Epoch [100/300], G Loss: 0.8121, D Loss: 1.2833\n",
      "Epoch [150/300], G Loss: 0.8120, D Loss: 1.2916\n",
      "Epoch [200/300], G Loss: 0.7933, D Loss: 1.2990\n",
      "Epoch [250/300], G Loss: 0.7746, D Loss: 1.3033\n",
      "Epoch [300/300], G Loss: 0.7929, D Loss: 1.3015\n",
      "Generating synthetic data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (500, 8), indices imply (500, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 372\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ctgan, data, synthetic_data\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 372\u001b[0m     example_usage()\n",
      "Cell \u001b[1;32mIn[1], line 320\u001b[0m, in \u001b[0;36mexample_usage\u001b[1;34m()\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# Generate synthetic data\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating synthetic data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 320\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39mgenerate(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Display some statistics\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal Data Statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 244\u001b[0m, in \u001b[0;36mSimpleCTGAN.generate\u001b[1;34m(self, n_samples)\u001b[0m\n\u001b[0;32m    241\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(synthetic_data)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m synthetic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(synthetic_data, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_columns\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# We need to post-process the data to get back the categorical variables\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Get the one-hot encoded columns for this category\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    828\u001b[0m             data,\n\u001b[0;32m    829\u001b[0m             index,\n\u001b[0;32m    830\u001b[0m             columns,\n\u001b[0;32m    831\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    832\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    833\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    834\u001b[0m         )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (500, 8), indices imply (500, 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[256, 256]):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.layers.append(nn.Tanh())  # Output values between -1 and 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.BatchNorm1d) and len(x) == 1:\n",
    "                continue  # Skip batch norm for single sample\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "            self.layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.layers.append(nn.Sigmoid())  # Output probability\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Simple CTGAN class\n",
    "class SimpleCTGAN:\n",
    "    def __init__(self, \n",
    "                 latent_dim=100, \n",
    "                 hidden_dims_gen=[256, 256],\n",
    "                 hidden_dims_disc=[256, 128],\n",
    "                 lr=0.0002,\n",
    "                 beta1=0.5,\n",
    "                 beta2=0.999):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.hidden_dims_gen = hidden_dims_gen\n",
    "        self.hidden_dims_disc = hidden_dims_disc\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        \n",
    "        # Track losses\n",
    "        self.g_losses = []\n",
    "        self.d_losses = []\n",
    "        \n",
    "        # Column info\n",
    "        self.columns = None\n",
    "        self.continuous_columns = None\n",
    "        self.categorical_columns = None\n",
    "        self.categorical_dimensions = None\n",
    "    \n",
    "    def fit(self, data, continuous_columns=None, categorical_columns=None, epochs=300, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the CTGAN on the provided data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: pandas DataFrame\n",
    "            The training data\n",
    "        continuous_columns: list\n",
    "            List of column names with continuous data\n",
    "        categorical_columns: list\n",
    "            List of column names with categorical data\n",
    "        epochs: int\n",
    "            Number of training epochs\n",
    "        batch_size: int\n",
    "            Batch size for training\n",
    "        \"\"\"\n",
    "        # Store column information\n",
    "        self.columns = data.columns.tolist()\n",
    "        \n",
    "        # Identify continuous and categorical columns if not provided\n",
    "        if continuous_columns is None and categorical_columns is None:\n",
    "            self.continuous_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            self.categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        else:\n",
    "            self.continuous_columns = continuous_columns if continuous_columns else []\n",
    "            self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        \n",
    "        # Process categorical variables\n",
    "        self.categorical_dimensions = {}\n",
    "        transformed_data = data.copy()\n",
    "        \n",
    "        for column in self.categorical_columns:\n",
    "            dummies = pd.get_dummies(data[column], prefix=column)\n",
    "            transformed_data = pd.concat([transformed_data.drop(column, axis=1), dummies], axis=1)\n",
    "            self.categorical_dimensions[column] = dummies.columns.tolist()\n",
    "        \n",
    "        # Only keep the relevant columns after transformation\n",
    "        processed_columns = self.continuous_columns.copy()\n",
    "        for column in self.categorical_columns:\n",
    "            processed_columns.extend(self.categorical_dimensions[column])\n",
    "        \n",
    "        transformed_data = transformed_data[processed_columns]\n",
    "        \n",
    "        # Scale the data\n",
    "        scaled_data = self.scaler.fit_transform(transformed_data)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        tensor_data = torch.FloatTensor(scaled_data).to(self.device)\n",
    "        dataset = TensorDataset(tensor_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize networks\n",
    "        output_dim = scaled_data.shape[1]\n",
    "        self.generator = Generator(self.latent_dim, output_dim, self.hidden_dims_gen).to(self.device)\n",
    "        self.discriminator = Discriminator(output_dim, self.hidden_dims_disc).to(self.device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        g_optimizer = optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_optimizer = optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            g_epoch_loss = 0.0\n",
    "            d_epoch_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "                batch_size = real_data.size(0)\n",
    "                \n",
    "                # Create labels\n",
    "                real_labels = torch.ones(batch_size, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size, 1).to(self.device)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # With real data\n",
    "                real_outputs = self.discriminator(real_data)\n",
    "                d_real_loss = criterion(real_outputs, real_labels)\n",
    "                \n",
    "                # With fake data\n",
    "                noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(noise)\n",
    "                fake_outputs = self.discriminator(fake_data.detach())\n",
    "                d_fake_loss = criterion(fake_outputs, fake_labels)\n",
    "                \n",
    "                # Combine losses and backpropagate\n",
    "                d_loss = d_real_loss + d_fake_loss\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_epoch_loss += d_loss.item()\n",
    "                \n",
    "                # Train Generator\n",
    "                g_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate fake data and try to fool discriminator\n",
    "                noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(noise)\n",
    "                fake_outputs = self.discriminator(fake_data)\n",
    "                \n",
    "                # Generator wants discriminator to classify fake as real\n",
    "                g_loss = criterion(fake_outputs, real_labels)\n",
    "                g_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                g_epoch_loss += g_loss.item()\n",
    "            \n",
    "            # Record losses\n",
    "            avg_g_loss = g_epoch_loss / len(dataloader)\n",
    "            avg_d_loss = d_epoch_loss / len(dataloader)\n",
    "            self.g_losses.append(avg_g_loss)\n",
    "            self.d_losses.append(avg_d_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}')\n",
    "    \n",
    "    def generate(self, n_samples=100):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_samples: int\n",
    "            Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas DataFrame with synthetic data\n",
    "        \"\"\"\n",
    "        if self.generator is None:\n",
    "            raise ValueError(\"Model hasn't been trained yet. Call fit() first.\")\n",
    "        \n",
    "        # Generate noise\n",
    "        noise = torch.randn(n_samples, self.latent_dim).to(self.device)\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        with torch.no_grad():\n",
    "            synthetic_data = self.generator(noise).cpu().numpy()\n",
    "        \n",
    "        # Inverse transform to get original scale\n",
    "        synthetic_data = self.scaler.inverse_transform(synthetic_data)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        synthetic_df = pd.DataFrame(synthetic_data, columns=self.continuous_columns.copy())\n",
    "        \n",
    "        # We need to post-process the data to get back the categorical variables\n",
    "        for column in self.categorical_columns:\n",
    "            # Get the one-hot encoded columns for this category\n",
    "            category_columns = self.categorical_dimensions[column]\n",
    "            \n",
    "            # Get the indices of these columns in the DataFrame\n",
    "            column_indices = [self.continuous_columns.index(cat_col) for cat_col in category_columns \n",
    "                             if cat_col in self.continuous_columns]\n",
    "            \n",
    "            if column_indices:\n",
    "                # Extract the values\n",
    "                category_values = synthetic_df.iloc[:, column_indices].values\n",
    "                \n",
    "                # Get the index of the max value for each row (one-hot encoding)\n",
    "                max_indices = np.argmax(category_values, axis=1)\n",
    "                \n",
    "                # Map back to the original categories\n",
    "                original_categories = [category_columns[idx].split('_')[-1] for idx in max_indices]\n",
    "                \n",
    "                # Add the column back to the DataFrame\n",
    "                synthetic_df[column] = original_categories\n",
    "                \n",
    "                # Remove the one-hot encoded columns\n",
    "                synthetic_df = synthetic_df.drop(columns=[category_columns[idx] for idx in column_indices])\n",
    "        \n",
    "        return synthetic_df\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot generator and discriminator losses\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.g_losses, label='Generator Loss')\n",
    "        plt.plot(self.d_losses, label='Discriminator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Losses')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    # Create some sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Continuous features\n",
    "    age = np.random.normal(loc=45, scale=15, size=n_samples)\n",
    "    income = np.random.lognormal(mean=10.5, sigma=0.5, size=n_samples)\n",
    "    \n",
    "    # Categorical features\n",
    "    gender = np.random.choice(['Male', 'Female'], size=n_samples)\n",
    "    education = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                               p=[0.4, 0.3, 0.2, 0.1], \n",
    "                               size=n_samples)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'Age': age,\n",
    "        'Income': income,\n",
    "        'Gender': gender,\n",
    "        'Education': education\n",
    "    })\n",
    "    \n",
    "    # Train the model\n",
    "    ctgan = SimpleCTGAN(latent_dim=64, hidden_dims_gen=[128, 128], hidden_dims_disc=[128, 64])\n",
    "    continuous_cols = ['Age', 'Income']\n",
    "    categorical_cols = ['Gender', 'Education']\n",
    "    \n",
    "    print(\"Training CTGAN model...\")\n",
    "    ctgan.fit(data, continuous_columns=continuous_cols, categorical_columns=categorical_cols, \n",
    "             epochs=300, batch_size=64)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    print(\"Generating synthetic data...\")\n",
    "    synthetic_data = ctgan.generate(n_samples=500)\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\nOriginal Data Statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    print(\"\\nSynthetic Data Statistics:\")\n",
    "    print(synthetic_data.describe())\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Age distribution\n",
    "    axs[0, 0].hist(data['Age'], bins=20, alpha=0.5, label='Original')\n",
    "    axs[0, 0].hist(synthetic_data['Age'], bins=20, alpha=0.5, label='Synthetic')\n",
    "    axs[0, 0].set_title('Age Distribution')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Income distribution\n",
    "    axs[0, 1].hist(data['Income'], bins=20, alpha=0.5, label='Original')\n",
    "    axs[0, 1].hist(synthetic_data['Income'], bins=20, alpha=0.5, label='Synthetic')\n",
    "    axs[0, 1].set_title('Income Distribution')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # Gender distribution\n",
    "    orig_gender_counts = data['Gender'].value_counts(normalize=True)\n",
    "    syn_gender_counts = synthetic_data['Gender'].value_counts(normalize=True)\n",
    "    \n",
    "    axs[1, 0].bar(orig_gender_counts.index, orig_gender_counts.values, alpha=0.5, label='Original')\n",
    "    axs[1, 0].bar(syn_gender_counts.index, syn_gender_counts.values, alpha=0.5, label='Synthetic')\n",
    "    axs[1, 0].set_title('Gender Distribution')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Education distribution\n",
    "    orig_edu_counts = data['Education'].value_counts(normalize=True)\n",
    "    syn_edu_counts = synthetic_data['Education'].value_counts(normalize=True)\n",
    "    \n",
    "    axs[1, 1].bar(orig_edu_counts.index, orig_edu_counts.values, alpha=0.5, label='Original')\n",
    "    axs[1, 1].bar(syn_edu_counts.index, syn_edu_counts.values, alpha=0.5, label='Synthetic')\n",
    "    axs[1, 1].set_title('Education Distribution')\n",
    "    axs[1, 1].set_xticklabels(orig_edu_counts.index, rotation=45)\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training losses\n",
    "    ctgan.plot_losses()\n",
    "    \n",
    "    return ctgan, data, synthetic_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Environment",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
