{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fa7948e-e64e-4785-b540-4fa8c44b6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau, CosineAnnealingLR, LambdaLR\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212d67bf-46da-42b1-ad08-cad3d4551dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruits_banane</th>\n",
       "      <th>fruits_orange</th>\n",
       "      <th>fruits_pomme</th>\n",
       "      <th>fruits_cat_pomme</th>\n",
       "      <th>fruits_cat_banane</th>\n",
       "      <th>fruits_cat_orange</th>\n",
       "      <th>fruits_cat_fraise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruits_banane  fruits_orange  fruits_pomme  fruits_cat_pomme  \\\n",
       "0          False          False          True              True   \n",
       "1           True          False         False             False   \n",
       "2          False          False          True              True   \n",
       "3          False           True         False             False   \n",
       "4           True          False         False             False   \n",
       "\n",
       "   fruits_cat_banane  fruits_cat_orange  fruits_cat_fraise  \n",
       "0              False              False              False  \n",
       "1               True              False              False  \n",
       "2              False              False              False  \n",
       "3              False               True              False  \n",
       "4               True              False              False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'fruits': ['pomme', 'banane', 'pomme', 'orange', 'banane']\n",
    "})\n",
    "\n",
    "# Catégoriser la colonne 'fruits' avec des catégories prédéfinies\n",
    "data['fruits_cat'] = pd.Categorical(data['fruits'], categories=['pomme', 'banane', 'orange', 'fraise'])\n",
    "pd.get_dummies(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "87017da0-6b5d-487a-b872-7947616d7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondVec:\n",
    "    def __init__(self, data, categorical_columns, categorical_dims):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.n_categories = sum(categorical_dims.values())\n",
    "        self.n_features = len(categorical_columns)\n",
    "        self.data = data\n",
    "        \n",
    "    def sample_conditional_vector(self, batch_size):\n",
    "        \"\"\"Sample conditional vectors for training.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None, None\n",
    "        \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        mask = np.zeros((batch_size, self.n_features), dtype='float32')\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Choose a random discrete column\n",
    "            feature_idx = np.random.choice(range(self.n_features))\n",
    "            feature = self.categorical_columns[feature_idx]\n",
    "            \n",
    "            # Choose a random category from that column\n",
    "            feature_dim = self.categorical_dims[feature]\n",
    "            category_idx = np.random.choice(range(feature_dim))\n",
    "            \n",
    "            # Set mask and vec values\n",
    "            mask[i, feature_idx] = 1\n",
    "            vec[i, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "            \n",
    "        return torch.from_numpy(vec), torch.from_numpy(mask)\n",
    "    \n",
    "    def generate_conditional_vector(self, conditions, batch_size):\n",
    "        \"\"\"Generate conditional vector based on conditions.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None\n",
    "            \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        for feature, category in conditions.items():\n",
    "            if feature in self.categorical_columns:\n",
    "                feature_idx = self.categorical_columns.index(feature)\n",
    "                category_idx = int(category)  # Assuming category is an index\n",
    "                \n",
    "                vec[:, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "        \n",
    "        return torch.from_numpy(vec)\n",
    "    \n",
    "class CTGANDataset(Dataset):\n",
    "    def __init__(self, data, categorical_columns=None):\n",
    "        self.data = data\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.continuous_columns = [col for col in data.columns if col not in self.categorical_columns]\n",
    "        \n",
    "        # Create encoders for categorical columns and fit GMMs for continuous columns\n",
    "        self.cond_vec = None\n",
    "        self.transformer = DataTransformer(self.categorical_columns)\n",
    "        self.transformer.fit(data)\n",
    "        self.transformed_data = self.transformer.transform(data)\n",
    "        \n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.cond_vec = CondVec(\n",
    "                data, \n",
    "                categorical_columns=self.categorical_columns,\n",
    "                categorical_dims=self.transformer.categorical_dims\n",
    "            )\n",
    "    def get_categorical_dims(self):\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            return self.transformer.categorical_dims\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transformed_data[idx]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample data and conditional vectors for training.\"\"\"\n",
    "        # Sample data\n",
    "        idx = np.random.choice(range(len(self)), batch_size)\n",
    "        data = self.transformed_data[idx]\n",
    "        \n",
    "        # Sample conditional vectors if categorical columns exist\n",
    "        if self.cond_vec:\n",
    "            cond_vec, mask = self.cond_vec.sample_conditional_vector(batch_size)\n",
    "            return data, cond_vec, mask\n",
    "        \n",
    "        return data, None, None\n",
    "    def train_val_split(self, val_ratio = 0.2):\n",
    "        return train_test_split(self.transformed_data, test_size=val_ratio)\n",
    "\n",
    "\n",
    "class TransformedCTGANDataset(Dataset):\n",
    "    def __init__(self, trans_data, categorical_columns=None, categorical_dims = 0 ):\n",
    "        self.transformed_data = trans_data\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.cond_vec = None\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.cond_vec = CondVec(\n",
    "                data, \n",
    "                categorical_columns=self.categorical_columns,\n",
    "                categorical_dims= categorical_dims\n",
    "            )    \n",
    "    def __len__(self):\n",
    "        return len(self.transformed_data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transformed_data[idx]\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample data and conditional vectors for training.\"\"\"\n",
    "        # Sample data\n",
    "        idx = np.random.choice(range(len(self)), batch_size)\n",
    "        data = self.transformed_data[idx]\n",
    "        \n",
    "        # Sample conditional vectors if categorical columns exist\n",
    "        if self.cond_vec:\n",
    "            cond_vec, mask = self.cond_vec.sample_conditional_vector(batch_size)\n",
    "            return data, cond_vec, mask\n",
    "        \n",
    "        return data, None, None\n",
    "        \n",
    "class DataTransformer:\n",
    "    \"\"\"Transforms data between original space and CTGAN transformed space.\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_columns):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.categorical_dims = {}\n",
    "        self.continuous_gmms = {}\n",
    "        self.n_clusters = 10  # Number of modes for GMM\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the data transformer.\"\"\"\n",
    "        # Process categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            categories = pd.Categorical(data[column]).categories\n",
    "            self.categorical_dims[column] = len(categories)\n",
    "            \n",
    "        # Process continuous columns by fitting GMMs\n",
    "        continuous_columns = [c for c in data.columns if c not in self.categorical_columns]\n",
    "        for column in continuous_columns:\n",
    "            col_data = data[column].values.reshape(-1, 1)\n",
    "            gmm = GaussianMixture(n_components=self.n_clusters)\n",
    "            gmm.fit(col_data)\n",
    "            self.continuous_gmms[column] = gmm\n",
    "            \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform data to CTGAN format.\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        # Transform categorical columns to one-hot encoding\n",
    "        for column in self.categorical_columns:\n",
    "            one_hot = pd.get_dummies(data[column], prefix=column)\n",
    "            result.append(one_hot.values)\n",
    "            \n",
    "        # Transform continuous columns with mode-specific normalization\n",
    "        for column in data.columns:\n",
    "            if column not in self.categorical_columns:\n",
    "                col_data = data[column].values.reshape(-1, 1)\n",
    "                gmm = self.continuous_gmms[column]\n",
    "                \n",
    "                # Get cluster assignments and probabilities\n",
    "                clusters = gmm.predict(col_data)\n",
    "                probs = gmm.predict_proba(col_data)\n",
    "                \n",
    "                # Normalize data based on Gaussian parameters\n",
    "                normalized = np.zeros_like(col_data)\n",
    "                for i in range(len(col_data)):\n",
    "                    cluster = clusters[i]\n",
    "                    mean = gmm.means_[cluster][0]\n",
    "                    std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                    normalized[i] = (col_data[i] - mean) / (4 * std)\n",
    "                \n",
    "                # Create encoded data: [normalized value, cluster_1_prob, ..., cluster_k_prob]\n",
    "                encoded = np.zeros((len(col_data), self.n_clusters + 1))\n",
    "                encoded[:, 0] = normalized.flatten()\n",
    "                encoded[:, 1:] = probs\n",
    "                \n",
    "                result.append(encoded)\n",
    "                \n",
    "        # Combine all transformed columns\n",
    "        if result:\n",
    "            return np.concatenate(result, axis=1).astype('float32')\n",
    "        return np.zeros((len(data), 0))\n",
    "        \n",
    "    def inverse_transform(self, transformed_data):\n",
    "        \"\"\"Convert transformed data back to original format.\"\"\"\n",
    "        # Create a DataFrame for the result\n",
    "        result = pd.DataFrame()\n",
    "        column_idx = 0\n",
    "        \n",
    "        # Inverse transform categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            dim = self.categorical_dims[column]\n",
    "            one_hot = transformed_data[:, column_idx:column_idx + dim]\n",
    "            \n",
    "            # Convert one-hot back to categorical\n",
    "            indices = np.argmax(one_hot, axis=1)\n",
    "            # Récupérer les catégories originales\n",
    "            try:\n",
    "                categories = pd.Categorical(self.data[column]).categories\n",
    "                result[column] = pd.Categorical.from_codes(indices, categories=categories)\n",
    "            except:\n",
    "                # Fallback en cas d'erreur\n",
    "                result[column] = indices\n",
    "            \n",
    "            column_idx += dim\n",
    "            \n",
    "        # Inverse transform continuous columns\n",
    "        for column in self.continuous_gmms:\n",
    "            gmm = self.continuous_gmms[column]\n",
    "            \n",
    "            # Extract normalized value and cluster probabilities\n",
    "            normalized = transformed_data[:, column_idx]\n",
    "            probs = transformed_data[:, column_idx + 1:column_idx + 1 + self.n_clusters]\n",
    "            \n",
    "            # Convert back to original space\n",
    "            cluster_idx = np.argmax(probs, axis=1)\n",
    "            values = np.zeros(len(normalized))\n",
    "            \n",
    "            for i in range(len(normalized)):\n",
    "                cluster = cluster_idx[i]\n",
    "                mean = gmm.means_[cluster][0]\n",
    "                std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                values[i] = normalized[i] * (4 * std) + mean\n",
    "                \n",
    "            result[column] = values\n",
    "            column_idx += self.n_clusters + 1\n",
    "            \n",
    "        return result\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_categories=0, hidden_dims=[256, 256]):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        dims = [input_dim + n_categories] + hidden_dims + [output_dim]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:\n",
    "                self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "                self.layers.append(nn.ReLU())\n",
    "        \n",
    "    def forward(self, noise, cond_vec=None):\n",
    "        if cond_vec is not None:\n",
    "            x = torch.cat([noise, cond_vec], dim=1)\n",
    "\n",
    "        else:\n",
    "            x = noise\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, n_categories=0, hidden_dims=[256, 128]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.pac = 1  # Default no pac\n",
    "        \n",
    "        # Placeholder for main layers - will be initialized in set_pac\n",
    "        self.main_layers = None\n",
    "        self.output_layer = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Store parameters for layer initialization\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_categories = n_categories\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Initialize layers with default pac=1\n",
    "        self._init_layers()\n",
    "            \n",
    "    def _init_layers(self):\n",
    "        \"\"\"Initialize network layers based on current pac value\"\"\"\n",
    "        pac_input_dim = self.input_dim * self.pac\n",
    "        \n",
    "        self.main_layers = nn.ModuleList()\n",
    "        for i in range(len(self.hidden_dims)):\n",
    "            if i == 0:\n",
    "                self.main_layers.append(nn.Linear(pac_input_dim, self.hidden_dims[i]))\n",
    "            else:\n",
    "                self.main_layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
    "            self.main_layers.append(nn.LeakyReLU(0.2))\n",
    "        self.main_layers.to(self.device)\n",
    "        self.output_layer = nn.Linear(self.hidden_dims[-1], 1, device= self.device)\n",
    "        \n",
    "        # Conditional embedding layers\n",
    "        self.cond_layers = None\n",
    "        if self.n_categories > 0:\n",
    "            \n",
    "            self.cond_layers = nn.Sequential(\n",
    "                nn.Linear(self.n_categories, pac_input_dim),\n",
    "                nn.ReLU()\n",
    "            ).to(self.device)\n",
    "    \n",
    "    def set_pac(self, pac):\n",
    "        \"\"\"Update the model to handle pac-sized inputs\"\"\"\n",
    "        self.pac = pac\n",
    "        self._init_layers()\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        if cond_vec is not None and self.cond_layers is not None:\n",
    "            cond = self.cond_layers(cond_vec)\n",
    "            x = x + cond\n",
    "            \n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class PacGan(nn.Module):\n",
    "    \"\"\"PacGAN discriminator for improved GAN training stability.\"\"\"\n",
    "    def __init__(self, discriminator, pac=10):\n",
    "        super(PacGan, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.pac = pac\n",
    "        # Tell the discriminator about pac to handle dimensions\n",
    "        if hasattr(self.discriminator, 'set_pac'):\n",
    "            self.discriminator.set_pac(pac)\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        batch_size = x.size(0)\n",
    "        if batch_size % self.pac != 0:\n",
    "            # Padding to make divisible by pac\n",
    "            pad_size = self.pac - (batch_size % self.pac)\n",
    "            indices = np.random.choice(batch_size, pad_size)\n",
    "            x = torch.cat([x, x[indices]], dim=0)\n",
    "            if cond_vec is not None:\n",
    "                cond_vec = torch.cat([cond_vec, cond_vec[indices]], dim=0)\n",
    "                \n",
    "        # Reshape x for PacGAN structure\n",
    "        new_batch_size = x.size(0) // self.pac\n",
    "        x_reshaped = x.view(new_batch_size, self.pac * x.size(1))\n",
    "        \n",
    "        # For conditional vectors, we need to have one per batch\n",
    "        if cond_vec is not None:\n",
    "            # Take one conditional vector per pac group\n",
    "            cond_vec_reshaped = cond_vec.view(new_batch_size, self.pac, cond_vec.size(1))\n",
    "            cond_vec_flat = cond_vec_reshaped[:, 0, :]  # Just take the first one\n",
    "            return self.discriminator(x_reshaped, cond_vec_flat)\n",
    "        else:\n",
    "            return self.discriminator(x_reshaped, None)\n",
    "\n",
    "class CTGAN:\n",
    "    def __init__(self, categorical_columns=None, noise_dim=100, batch_size=500, \n",
    "                 generator_lr=2e-4, discriminator_lr=2e-4, pac=10):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.noise_dim = noise_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.generator_lr = generator_lr\n",
    "        self.discriminator_lr = discriminator_lr\n",
    "        self.pac = pac\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.transformer = None\n",
    "        self.dataset = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        \n",
    "    def fit(self, data, val_size= 0.2, epochs=300):\n",
    "        \"\"\"Fit CTGAN to the data with train and validation datasets and learning rate scheduling\"\"\"\n",
    "       \n",
    "        # Create datasets\n",
    "        self.dataset = CTGANDataset(data, categorical_columns=self.categorical_columns)\n",
    "        categorical_dims = self.dataset.get_categorical_dims()\n",
    "        #########################################################################################################################################\n",
    "        train, val = self.dataset.train_val_split(val_ratio= 0.2)\n",
    "        \n",
    "        self.train = TransformedCTGANDataset(train, categorical_columns=self.categorical_columns, categorical_dims=categorical_dims)\n",
    "        self.val = TransformedCTGANDataset(val, categorical_columns=self.categorical_columns, categorical_dims=categorical_dims)\n",
    "        \n",
    "        # Use the same transformer for both datasets\n",
    "        self.transformer = self.dataset.transformer\n",
    "        \n",
    "        # Calculate dimensions\n",
    "        data_dim = self.dataset.transformed_data.shape[1]\n",
    "        n_categories = 0\n",
    "        if self.train.cond_vec:\n",
    "            n_categories = self.dataset.cond_vec.n_categories\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim, \n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        generator_optimizer = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=self.generator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        discriminator_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=self.discriminator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        # Setup learning rate schedulers with proper optimizer references\n",
    "        generator_scheduler = ReduceLROnPlateau(\n",
    "            generator_optimizer, \n",
    "            'min', \n",
    "            patience=10, \n",
    "            factor=0.2\n",
    "        )\n",
    "        \n",
    "        discriminator_scheduler = ReduceLROnPlateau(\n",
    "            discriminator_optimizer, \n",
    "            'min', \n",
    "            patience=10, \n",
    "            factor=0.2\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Ensure batch size is a multiple of pac for PacGAN\n",
    "        batch_size = (self.batch_size // self.pac) * self.pac\n",
    "        if batch_size == 0:\n",
    "            batch_size = self.pac\n",
    "            \n",
    "        run = wandb.init(project=\"Data Augmentation - CTGAN\", name=\"ctgan\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": self.batch_size,\n",
    "        \"generator_lr\": self.generator_lr,\n",
    "        \"discriminator_lr\": self.discriminator_lr,\n",
    "        \"pac\": self.pac,\n",
    "        \"noise_dim\": self.noise_dim,\n",
    "        \"categorical_columns\": self.categorical_columns,\n",
    "        \"data_dim\": data_dim,\n",
    "        \"n_categories\": n_categories\n",
    "    })\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            \n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            for _ in range(max(1, len(self.train) // batch_size)):\n",
    "                # Get real data and conditional vectors from training set\n",
    "                real_data, cond_vec, mask = self.train.sample(batch_size)\n",
    "                real_data = torch.from_numpy(real_data).to(self.device)\n",
    "                \n",
    "                if cond_vec is not None:\n",
    "                    \n",
    "                    #cond_vec = torch.from_numpy(cond_vec).to(self.device)\n",
    "                    cond_vec = cond_vec.to(self.device)\n",
    "                    #mask = torch.from_numpy(mask).to(self.device)\n",
    "                    mask = mask.to(self.device)\n",
    "                \n",
    "                # Labels for real and fake data\n",
    "                real_labels = torch.ones(batch_size // self.pac, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size // self.pac, 1).to(self.device)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                discriminator_optimizer.zero_grad()\n",
    "                \n",
    "                # Real data loss\n",
    "                outputs = self.discriminator(real_data, cond_vec)\n",
    "                d_real_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                # Generate fake data\n",
    "                noise = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
    "                fake_data = self.generator(noise, cond_vec)\n",
    "                \n",
    "                outputs = self.discriminator(fake_data.detach(), cond_vec)\n",
    "                d_fake_loss = criterion(outputs, fake_labels)\n",
    "                \n",
    "                d_loss = d_real_loss + d_fake_loss\n",
    "                d_loss.backward()\n",
    "                discriminator_optimizer.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                generator_optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.discriminator(fake_data, cond_vec)\n",
    "                g_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Validation phase\n",
    "            self.generator.eval()\n",
    "            self.discriminator.eval()\n",
    "            \n",
    "            g_val_losses = []\n",
    "            d_val_losses = []\n",
    "            d_val_real_losses = []\n",
    "            d_val_fake_losses = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(max(1, len(self.val) // batch_size)):\n",
    "                    # Get real data and conditional vectors from validation set\n",
    "                    real_val_data, val_cond_vec, val_mask = self.val.sample(batch_size)\n",
    "                    real_val_data = torch.from_numpy(real_val_data).to(self.device)\n",
    "                    \n",
    "                    if val_cond_vec is not None:\n",
    "                        #val_cond_vec = torch.from_numpy(val_cond_vec).to(self.device)\n",
    "                        #val_mask = torch.from_numpy(val_mask).to(self.device)\n",
    "                        val_cond_vec = val_cond_vec.to(self.device)\n",
    "                        val_mask = val_mask.to(self.device)\n",
    "                        \n",
    "                    \n",
    "                    # Labels for real and fake validation data\n",
    "                    real_labels = torch.ones(batch_size // self.pac, 1).to(self.device)\n",
    "                    fake_labels = torch.zeros(batch_size // self.pac, 1).to(self.device)\n",
    "                    \n",
    "                    # Validation - Discriminator\n",
    "                    val_outputs = self.discriminator(real_val_data, val_cond_vec)\n",
    "                    d_val_real_loss = criterion(val_outputs, real_labels)\n",
    "                    d_val_real_losses.append(d_val_real_loss.item())\n",
    "                    \n",
    "                    # Generate fake validation data\n",
    "                    val_noise = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
    "                    val_fake_data = self.generator(val_noise, val_cond_vec)\n",
    "                    \n",
    "                    val_outputs = self.discriminator(val_fake_data, val_cond_vec)\n",
    "                    d_val_fake_loss = criterion(val_outputs, fake_labels)\n",
    "                    d_val_fake_losses.append(d_val_fake_loss.item())\n",
    "                    \n",
    "                    d_val_loss = d_val_real_loss + d_val_fake_loss\n",
    "                    \n",
    "                    # Validation - Generator\n",
    "                    g_val_loss = criterion(val_outputs, real_labels)\n",
    "                    \n",
    "                    g_val_losses.append(g_val_loss.item())\n",
    "                    d_val_losses.append(d_val_loss.item())\n",
    "            \n",
    "            # Calculate average losses\n",
    "            avg_g_loss = np.mean(g_losses)\n",
    "            avg_d_loss = np.mean(d_losses)\n",
    "            avg_g_val_loss = np.mean(g_val_losses)\n",
    "            avg_d_val_loss = np.mean(d_val_losses)\n",
    "            avg_d_val_real_loss = np.mean(d_val_real_losses)\n",
    "            avg_d_val_fake_loss = np.mean(d_val_fake_losses)\n",
    "            \n",
    "            # Update learning rates based on validation losses\n",
    "            generator_scheduler.step(avg_g_val_loss)\n",
    "            discriminator_scheduler.step(avg_d_val_loss)\n",
    "            \n",
    "            # Get current learning rates\n",
    "            g_lr = generator_optimizer.param_groups[0]['lr']\n",
    "            d_lr = discriminator_optimizer.param_groups[0]['lr']\n",
    "            wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"g_loss\": avg_g_loss,\n",
    "            \"d_loss\": avg_d_loss,\n",
    "            \"g_val_loss\": avg_g_val_loss,\n",
    "            \"d_val_loss\": avg_d_val_loss,\n",
    "            \"d_val_real_loss\": avg_d_val_real_loss,\n",
    "            \"d_val_fake_loss\": avg_d_val_fake_loss,\n",
    "            \"generator_lr\": g_lr,\n",
    "            \"discriminator_lr\": d_lr\n",
    "        })\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                      f\"Train - G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}, \"\n",
    "                      f\"Val - G Loss: {avg_g_val_loss:.4f}, D Loss: {avg_d_val_loss:.4f}, \"\n",
    "                      f\"LR - Generator: {g_lr:.6f}, Discriminator: {d_lr:.6f}\")\n",
    "        wandb.finish()\n",
    "    def generate(self, n_samples, conditions= None):\n",
    "        \"\"\"Generate synthetic samples with optional conditioning.\"\"\"\n",
    "        if self.generator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        self.generator.eval()\n",
    "        \n",
    "        steps = n_samples // self.batch_size + 1\n",
    "        data = []\n",
    "        \n",
    "        for i in range(steps):\n",
    "            n_batch = min(self.batch_size, n_samples - i * self.batch_size)\n",
    "            if n_batch <= 0:\n",
    "                break\n",
    "                \n",
    "            # Generate noise\n",
    "            noise = torch.randn(n_batch, self.noise_dim).to(self.device)\n",
    "            \n",
    "            # Generate conditional vector if necessary\n",
    "            cond_vec = None\n",
    "            if self.dataset.cond_vec and conditions:\n",
    "                cond_vec = self.dataset.cond_vec.generate_conditional_vector(conditions, n_batch)\n",
    "                cond_vec = cond_vec.to(self.device)\n",
    "                \n",
    "            # Generate data\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                fake = self.generator(noise, cond_vec)\n",
    "            data.append(fake.cpu().numpy())\n",
    "            \n",
    "        data = np.concatenate(data, axis=0)\n",
    "        \n",
    "        # Convert to the original data format\n",
    "        synthetic_data = self.transformer.inverse_transform(data[:n_samples])\n",
    "        return synthetic_data\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        if self.generator is None or self.discriminator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        state = {\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'transformer': self.transformer\n",
    "        }\n",
    "        \n",
    "        torch.save(state, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        state = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.noise_dim = state['noise_dim']\n",
    "        self.categorical_columns = state['categorical_columns']\n",
    "        self.transformer = state['transformer']\n",
    "        \n",
    "        # Recreate the dataset and models\n",
    "        n_categories = 0\n",
    "        data_dim = 0\n",
    "        \n",
    "        if hasattr(self.transformer, 'categorical_dims'):\n",
    "            n_categories = sum(self.transformer.categorical_dims.values())\n",
    "            if hasattr(self.transformer, 'continuous_gmms'):\n",
    "                continuous_dims = sum([gmm.n_components + 1 for gmm in self.transformer.continuous_gmms.values()])\n",
    "                data_dim = n_categories + continuous_dims\n",
    "        \n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim,\n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        self.generator.load_state_dict(state['generator'])\n",
    "        self.discriminator.load_state_dict(state['discriminator'])\n",
    "        \n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc59e6-70a4-4b52-9af6-beef7f2bfae3",
   "metadata": {},
   "source": [
    "# Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db188c46-467a-4aaa-a35a-1690d508e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\anhqu\\Desktop\\Anh Quan\\Data Augmentation\\Code\\wandb\\run-20250323_181309-30twtywx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/30twtywx' target=\"_blank\">ctgan</a></strong> to <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/30twtywx' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/30twtywx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train - G Loss: 2.6241, D Loss: 1.2483, Val - G Loss: 1.0804, D Loss: 1.0014, LR - Generator: 0.005000, Discriminator: 0.002000\n",
      "Epoch [10/50], Train - G Loss: 1.0461, D Loss: 1.4410, Val - G Loss: 0.6173, D Loss: 1.4706, LR - Generator: 0.005000, Discriminator: 0.002000\n",
      "Epoch [20/50], Train - G Loss: 0.7643, D Loss: 1.3422, Val - G Loss: 0.7269, D Loss: 1.3165, LR - Generator: 0.001000, Discriminator: 0.000400\n",
      "Epoch [30/50], Train - G Loss: 1.1724, D Loss: 0.8997, Val - G Loss: 0.9596, D Loss: 1.0699, LR - Generator: 0.000200, Discriminator: 0.000080\n",
      "Epoch [40/50], Train - G Loss: 1.3175, D Loss: 0.7020, Val - G Loss: 1.2621, D Loss: 0.7170, LR - Generator: 0.000040, Discriminator: 0.000016\n",
      "Epoch [50/50], Train - G Loss: 0.9679, D Loss: 0.9943, Val - G Loss: 0.9619, D Loss: 1.0025, LR - Generator: 0.000008, Discriminator: 0.000003\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>▆▄▇██▇▇████▇▇▇▇▇▇▇▆▇▄▂▁▂▄▇▇▆▆▅▂▂▂▃▃▄▄▅▅▄</td></tr><tr><td>d_val_fake_loss</td><td>▂█▄▃▅▄▅▄▆▃▄▃▄▄▄▄▄▃▃▄▁▁▁▂▃▄▄▃▃▃▁▁▂▂▂▂▂▃▂▂</td></tr><tr><td>d_val_loss</td><td>▃▄█▅▄▅▄▅▆▄▅▄▅▅▄▅▄▄▅▄▁▁▂▃▄▅▄▄▄▂▁▁▂▂▂▃▃▃▃▃</td></tr><tr><td>d_val_real_loss</td><td>▅▆█▇▇▅▆▃▆▇▆▆▆▇▆▆▅▆▅▅▆▂▁▁▂▆▆▆▅▅▂▂▂▂▂▃▄▄▄▄</td></tr><tr><td>discriminator_lr</td><td>██████████▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>g_loss</td><td>▅█▃▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▂▁▁▁▁▁▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>g_val_loss</td><td>▅▃▁▃▄▂▃▂▂▂▃▃▃▃▃▃▃▃▄▄▇█▇▆▄▃▃▃▄▄▇▆▆▆▅▅▅▄▄▄</td></tr><tr><td>generator_lr</td><td>███████████▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>0.99427</td></tr><tr><td>d_val_fake_loss</td><td>0.49145</td></tr><tr><td>d_val_loss</td><td>1.00247</td></tr><tr><td>d_val_real_loss</td><td>0.51102</td></tr><tr><td>discriminator_lr</td><td>0.0</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>g_loss</td><td>0.96789</td></tr><tr><td>g_val_loss</td><td>0.96188</td></tr><tr><td>generator_lr</td><td>1e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ctgan</strong> at: <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/30twtywx' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/30twtywx</a><br> View project at: <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250323_181309-30twtywx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real data statistics:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   MedHouseVal  HouseAge_Cat  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558      2.196560  \n",
      "std       10.386050      2.135952      2.003532      1.153956      1.229386  \n",
      "min        0.692308     32.540000   -124.350000      0.149990      0.000000  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000      1.000000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000      2.000000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250      3.000000  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010      4.000000  \n",
      "\n",
      "Conditioned data (HouseAge_Cat = 2) statistics:\n",
      "       HouseAge_Cat    MedInc   HouseAge  AveRooms  AveBedrms   Population  \\\n",
      "count      7.000000  7.000000   7.000000  7.000000   7.000000     7.000000   \n",
      "mean       1.714286  3.603007  15.860130  5.628700   1.043201  1251.966376   \n",
      "std        0.755929  0.821151   0.521143  0.637827   0.035975   513.752780   \n",
      "min        1.000000  2.453526  15.596905  4.659547   1.013855   821.856344   \n",
      "25%        1.000000  3.163381  15.621570  5.347538   1.024451   834.103686   \n",
      "50%        2.000000  3.213082  15.679197  5.442417   1.031111   882.742717   \n",
      "75%        2.000000  4.236737  15.733594  6.056393   1.044812  1741.664146   \n",
      "max        3.000000  4.754208  17.034483  6.491073   1.118913  1907.629908   \n",
      "\n",
      "       AveOccup   Latitude   Longitude  MedHouseVal  \n",
      "count  7.000000   7.000000    7.000000     7.000000  \n",
      "mean   2.888203  34.423291 -119.246902     2.247963  \n",
      "std    0.381446   1.598067    2.512443     1.212660  \n",
      "min    2.413746  32.867649 -124.084723     1.611196  \n",
      "25%    2.559996  33.968830 -119.768525     1.621201  \n",
      "50%    3.049003  34.070122 -118.254151     1.929599  \n",
      "75%    3.120709  34.090910 -117.872189     1.992170  \n",
      "max    3.393262  37.905789 -117.108012     4.968207  \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def example_usage():\n",
    "    # Sample tabular data\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    data = fetch_california_housing(as_frame=True).frame\n",
    "    \n",
    "    # For this example, let's convert HouseAge to categorical by binning\n",
    "    data['HouseAge_Cat'] = pd.cut(data['HouseAge'], bins=5, labels=False)\n",
    "    categorical_columns = ['HouseAge_Cat']\n",
    "    # Initialize CTGAN\n",
    "    ctgan = CTGAN(categorical_columns=categorical_columns, pac=5, generator_lr=0.005, discriminator_lr=0.002, batch_size= 500)  # Ajusté le pac à 5 pour éviter des problèmes de dimensionnalité\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"Training CTGAN model...\")\n",
    "    \n",
    "    ctgan.fit(data , epochs=50)\n",
    "    \n",
    "    # Generate synthetic data (unconditional)\n",
    "    #print(\"Generating synthetic data...\")\n",
    "    #synthetic_data = ctgan.generate(n_samples=1000)\n",
    "    \n",
    "    # Generate synthetic data with conditions\n",
    "    # Example condition: HouseAge_Cat = 2\"\"\"\n",
    "    conditioned_data = ctgan.generate(\n",
    "        n_samples=7,\n",
    "        conditions={'HouseAge_Cat': 2}\n",
    "    )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"\\nReal data statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    #print(\"\\nSynthetic data statistics:\")\n",
    "    #print(synthetic_data.describe())\n",
    "    \n",
    "    print(\"\\nConditioned data (HouseAge_Cat = 2) statistics:\")\n",
    "    print(conditioned_data.describe())\n",
    "    \n",
    "    # Save model\n",
    "\"\"\"    ctgan.save(\"ctgan_model.pt\")\n",
    "    \n",
    "    # Load model\n",
    "    new_ctgan = CTGAN()\n",
    "    new_ctgan.load(\"ctgan_model.pt\")\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77bd8342-b732-4dc3-9a9a-8707086462a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_california_housing(as_frame=True).frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06960d-3217-43c2-af56-d154d5b0c5d6",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8909627b-2bb3-461b-ac60-baa361fa3b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_samples >= n_components but got n_components = 10, n_samples = 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m synthetic_data\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m     example_usage()\n",
      "Cell \u001b[1;32mIn[53], line 19\u001b[0m, in \u001b[0;36mexample_usage\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining CTGAN model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m train, val \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m ctgan\u001b[38;5;241m.\u001b[39mfit(train,val, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Generate synthetic data with conditions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Example condition: HouseAge_Cat = 2\u001b[39;00m\n\u001b[0;32m     24\u001b[0m conditioned_data \u001b[38;5;241m=\u001b[39m ctgan\u001b[38;5;241m.\u001b[39mgenerate( n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m, conditions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m} )\n",
      "Cell \u001b[1;32mIn[45], line 329\u001b[0m, in \u001b[0;36mCTGAN.fit\u001b[1;34m(self, train, val, epochs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit CTGAN to the data with train and validation datasets and learning rate scheduling\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m CTGANDataset(train, categorical_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval \u001b[38;5;241m=\u001b[39m CTGANDataset(val, categorical_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Use the same transformer for both datasets\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[45], line 56\u001b[0m, in \u001b[0;36mCTGANDataset.__init__\u001b[1;34m(self, data, categorical_columns)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m DataTransformer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[45], line 106\u001b[0m, in \u001b[0;36mDataTransformer.fit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m col_data \u001b[38;5;241m=\u001b[39m data[column]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    105\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[1;32m--> 106\u001b[0m gmm\u001b[38;5;241m.\u001b[39mfit(col_data)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_gmms[column] \u001b[38;5;241m=\u001b[39m gmm\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:180\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_predict(X, y)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\sklearn\\mixture\\_base.py:213\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\u001b[38;5;28mself\u001b[39m, X, dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32], ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_samples >= n_components \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got n_components = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_parameters(X)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# if we enable warm_start, we will have a unique initialisation\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_samples >= n_components but got n_components = 10, n_samples = 7"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def example_usage():\n",
    "    # Sample tabular data\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.DataFrame({'a': [1,2,3,4,5,6,7,8,9,10], 'b': [1,-1,-1,1,1,1,1,1,1,-1 ]})\n",
    "    data['b'] = pd.Categorical(data['b'])\n",
    "    pd.Categorical(data['b']).categories\n",
    "    categorical_columns = ['b']\n",
    "    # Initialize CTGAN\n",
    "    ctgan = CTGAN(categorical_columns=categorical_columns, pac=5, noise_dim=1, batch_size=1)  # Ajusté le pac à 5 pour éviter des problèmes de dimensionnalité\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"Training CTGAN model...\")\n",
    "    train, val = train_test_split(data, test_size=0.3)\n",
    "    \n",
    "    ctgan.fit(train,val, epochs=1)\n",
    "\n",
    "    \n",
    "    # Generate synthetic data with conditions\n",
    "    # Example condition: HouseAge_Cat = 2\n",
    "    conditioned_data = ctgan.generate( n_samples=11, conditions = {'b': -1} )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"\\nReal data statistics:\")\n",
    "    display(data)\n",
    "    print(data.describe())\n",
    "    \n",
    "    print(\"\\nConditioned data :\")\n",
    "    display(conditioned_data)\n",
    "    print(conditioned_data.describe())\n",
    "    \n",
    "    # Save model\n",
    "    return synthetic_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b856ea39-d2a0-4694-9116-7eb2d81ccb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CTGANDataset(data, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e2f4ca-5ba7-4ee1-8ad9-e6d22a192685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['z', 'zz', 'zzz'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame({'a': [1,2,3,4,5,6,7,8,9,10], 'b': ['z','zz','zz','z','zzz','z','zzz','zzz','z','zzz' ]})\n",
    "a['b'] = pd.Categorical(a['b'])\n",
    "pd.Categorical(a['b']).categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e34cc4e-facf-4542-919b-6f98c3704c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32),\n",
       " tensor([[0., 1., 0.]]),\n",
       " tensor([[1.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = CTGANDataset(a, categorical_columns= ['b'])\n",
    "aa.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79b3a27c-2c2a-4775-b4d4-e5862138b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.1629847e-11,\n",
       "        9.9996722e-01, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 3.2581773e-02,\n",
       "        6.3743941e-02, 1.0747959e-42],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 6.6141941e-02,\n",
       "        4.1500479e-02, 3.8707367e-40],\n",
       "       ...,\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 8.7430551e-26,\n",
       "        2.6141222e-21, 5.6873501e-04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 1.5613448e-27,\n",
       "        3.4088894e-22, 6.9994559e-05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, ..., 1.9312942e-26,\n",
       "        1.2193060e-21, 2.6615727e-04]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c15ca-9c83-47b8-aa58-dd49b8818871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = self.dataset.transformed_data.shape[1]\n",
    "        n_categories = 0\n",
    "        if self.dataset.cond_vec:\n",
    "            n_categories = self.dataset.cond_vec.n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8435335-41a7-412e-8574-cae5329aa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTransformer(['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ad093747-dff0-4def-b592-80d75bf2a6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dt\u001b[38;5;241m.\u001b[39mtransform(a)\n",
      "Cell \u001b[1;32mIn[49], line 122\u001b[0m, in \u001b[0;36mDataTransformer.transform\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns:\n\u001b[0;32m    121\u001b[0m     col_data \u001b[38;5;241m=\u001b[39m data[column]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m     gmm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_gmms[column]\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# Get cluster assignments and probabilities\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(col_data)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "dt.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6c7a4-e302-47da-92ce-8dd9dd32e4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Environment",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
