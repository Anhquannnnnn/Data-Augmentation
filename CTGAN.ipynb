{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fa7948e-e64e-4785-b540-4fa8c44b6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ExponentialLR, ReduceLROnPlateau, CosineAnnealingLR, LambdaLR\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212d67bf-46da-42b1-ad08-cad3d4551dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruits_banane</th>\n",
       "      <th>fruits_orange</th>\n",
       "      <th>fruits_pomme</th>\n",
       "      <th>fruits_cat_pomme</th>\n",
       "      <th>fruits_cat_banane</th>\n",
       "      <th>fruits_cat_orange</th>\n",
       "      <th>fruits_cat_fraise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruits_banane  fruits_orange  fruits_pomme  fruits_cat_pomme  \\\n",
       "0          False          False          True              True   \n",
       "1           True          False         False             False   \n",
       "2          False          False          True              True   \n",
       "3          False           True         False             False   \n",
       "4           True          False         False             False   \n",
       "\n",
       "   fruits_cat_banane  fruits_cat_orange  fruits_cat_fraise  \n",
       "0              False              False              False  \n",
       "1               True              False              False  \n",
       "2              False              False              False  \n",
       "3              False               True              False  \n",
       "4               True              False              False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'fruits': ['pomme', 'banane', 'pomme', 'orange', 'banane']\n",
    "})\n",
    "\n",
    "# Catégoriser la colonne 'fruits' avec des catégories prédéfinies\n",
    "data['fruits_cat'] = pd.Categorical(data['fruits'], categories=['pomme', 'banane', 'orange', 'fraise'])\n",
    "pd.get_dummies(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87017da0-6b5d-487a-b872-7947616d7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondVec:\n",
    "    def __init__(self, data, categorical_columns, categorical_dims):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.n_categories = sum(categorical_dims.values())\n",
    "        self.n_features = len(categorical_columns)\n",
    "        self.data = data\n",
    "        \n",
    "    def sample_conditional_vector(self, batch_size):\n",
    "        \"\"\"Sample conditional vectors for training.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None, None\n",
    "        \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        mask = np.zeros((batch_size, self.n_features), dtype='float32')\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Choose a random discrete column\n",
    "            feature_idx = np.random.choice(range(self.n_features))\n",
    "            feature = self.categorical_columns[feature_idx]\n",
    "            \n",
    "            # Choose a random category from that column\n",
    "            feature_dim = self.categorical_dims[feature]\n",
    "            category_idx = np.random.choice(range(feature_dim))\n",
    "            \n",
    "            # Set mask and vec values\n",
    "            mask[i, feature_idx] = 1\n",
    "            vec[i, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "            \n",
    "        return torch.from_numpy(vec), torch.from_numpy(mask)\n",
    "    \n",
    "    def generate_conditional_vector(self, conditions, batch_size):\n",
    "        \"\"\"Generate conditional vector based on conditions.\"\"\"\n",
    "        if self.n_features == 0:\n",
    "            return None\n",
    "            \n",
    "        vec = np.zeros((batch_size, self.n_categories), dtype='float32')\n",
    "        for feature, category in conditions.items():\n",
    "            if feature in self.categorical_columns:\n",
    "                feature_idx = self.categorical_columns.index(feature)\n",
    "                category_idx = int(category)  # Assuming category is an index\n",
    "                \n",
    "                vec[:, sum(list(self.categorical_dims.values())[:feature_idx]) + category_idx] = 1\n",
    "        \n",
    "        return torch.from_numpy(vec)\n",
    "    \n",
    "class CTGANDataset(Dataset):\n",
    "    def __init__(self, data, categorical_columns=None):\n",
    "        self.data = data\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.continuous_columns = [col for col in data.columns if col not in self.categorical_columns]\n",
    "        \n",
    "        # Create encoders for categorical columns and fit GMMs for continuous columns\n",
    "        self.cond_vec = None\n",
    "        self.transformer = DataTransformer(self.categorical_columns)\n",
    "        self.transformer.fit(data)\n",
    "        self.transformed_data = self.transformer.transform(data)\n",
    "        \n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.cond_vec = CondVec(\n",
    "                data, \n",
    "                categorical_columns=self.categorical_columns,\n",
    "                categorical_dims=self.transformer.categorical_dims\n",
    "            )\n",
    "    def get_categorical_dims(self):\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            return self.transformer.categorical_dims\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transformed_data[idx]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample data and conditional vectors for training.\"\"\"\n",
    "        # Sample data\n",
    "        idx = np.random.choice(range(len(self)), batch_size)\n",
    "        data = self.transformed_data[idx]\n",
    "        \n",
    "        # Sample conditional vectors if categorical columns exist\n",
    "        if self.cond_vec:\n",
    "            cond_vec, mask = self.cond_vec.sample_conditional_vector(batch_size)\n",
    "            return data, cond_vec, mask\n",
    "        \n",
    "        return data, None, None\n",
    "    def train_val_split(self, val_ratio = 0.2):\n",
    "        return train_test_split(self.transformed_data, test_size=val_ratio)\n",
    "\n",
    "\n",
    "class TransformedCTGANDataset(Dataset):\n",
    "    def __init__(self, trans_data, categorical_columns=None, categorical_dims = 0 ):\n",
    "        self.transformed_data = trans_data\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.cond_vec = None\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            self.cond_vec = CondVec(\n",
    "                data, \n",
    "                categorical_columns=self.categorical_columns,\n",
    "                categorical_dims= categorical_dims\n",
    "            )    \n",
    "    def __len__(self):\n",
    "        return len(self.transformed_data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transformed_data[idx]\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample data and conditional vectors for training.\"\"\"\n",
    "        # Sample data\n",
    "        idx = np.random.choice(range(len(self)), batch_size)\n",
    "        data = self.transformed_data[idx]\n",
    "        \n",
    "        # Sample conditional vectors if categorical columns exist\n",
    "        if self.cond_vec:\n",
    "            cond_vec, mask = self.cond_vec.sample_conditional_vector(batch_size)\n",
    "            return data, cond_vec, mask\n",
    "        \n",
    "        return data, None, None\n",
    "        \n",
    "class DataTransformer:\n",
    "    \"\"\"Transforms data between original space and CTGAN transformed space.\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_columns):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.categorical_dims = {}\n",
    "        self.continuous_gmms = {}\n",
    "        self.n_clusters = 10  # Number of modes for GMM\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the data transformer.\"\"\"\n",
    "        # Process categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            categories = pd.Categorical(data[column]).categories\n",
    "            self.categorical_dims[column] = len(categories)\n",
    "            \n",
    "        # Process continuous columns by fitting GMMs\n",
    "        continuous_columns = [c for c in data.columns if c not in self.categorical_columns]\n",
    "        for column in continuous_columns:\n",
    "            col_data = data[column].values.reshape(-1, 1)\n",
    "            gmm = GaussianMixture(n_components=self.n_clusters)\n",
    "            gmm.fit(col_data)\n",
    "            self.continuous_gmms[column] = gmm\n",
    "            \n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform data to CTGAN format.\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        # Transform categorical columns to one-hot encoding\n",
    "        for column in self.categorical_columns:\n",
    "            one_hot = pd.get_dummies(data[column], prefix=column)\n",
    "            result.append(one_hot.values)\n",
    "            \n",
    "        # Transform continuous columns with mode-specific normalization\n",
    "        for column in data.columns:\n",
    "            if column not in self.categorical_columns:\n",
    "                col_data = data[column].values.reshape(-1, 1)\n",
    "                gmm = self.continuous_gmms[column]\n",
    "                \n",
    "                # Get cluster assignments and probabilities\n",
    "                clusters = gmm.predict(col_data)\n",
    "                probs = gmm.predict_proba(col_data)\n",
    "                \n",
    "                # Normalize data based on Gaussian parameters\n",
    "                normalized = np.zeros_like(col_data)\n",
    "                for i in range(len(col_data)):\n",
    "                    cluster = clusters[i]\n",
    "                    mean = gmm.means_[cluster][0]\n",
    "                    std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                    normalized[i] = (col_data[i] - mean) / (4 * std)\n",
    "                \n",
    "                # Create encoded data: [normalized value, cluster_1_prob, ..., cluster_k_prob]\n",
    "                encoded = np.zeros((len(col_data), self.n_clusters + 1))\n",
    "                encoded[:, 0] = normalized.flatten()\n",
    "                encoded[:, 1:] = probs\n",
    "                \n",
    "                result.append(encoded)\n",
    "                \n",
    "        # Combine all transformed columns\n",
    "        if result:\n",
    "            return np.concatenate(result, axis=1).astype('float32')\n",
    "        return np.zeros((len(data), 0))\n",
    "        \n",
    "    def inverse_transform(self, transformed_data):\n",
    "        \"\"\"Convert transformed data back to original format.\"\"\"\n",
    "        # Create a DataFrame for the result\n",
    "        result = pd.DataFrame()\n",
    "        column_idx = 0\n",
    "        \n",
    "        # Inverse transform categorical columns\n",
    "        for column in self.categorical_columns:\n",
    "            dim = self.categorical_dims[column]\n",
    "            one_hot = transformed_data[:, column_idx:column_idx + dim]\n",
    "            \n",
    "            # Convert one-hot back to categorical\n",
    "            indices = np.argmax(one_hot, axis=1)\n",
    "            # Récupérer les catégories originales\n",
    "            try:\n",
    "                categories = pd.Categorical(self.data[column]).categories\n",
    "                result[column] = pd.Categorical.from_codes(indices, categories=categories)\n",
    "            except:\n",
    "                # Fallback en cas d'erreur\n",
    "                result[column] = indices\n",
    "            \n",
    "            column_idx += dim\n",
    "            \n",
    "        # Inverse transform continuous columns\n",
    "        for column in self.continuous_gmms:\n",
    "            gmm = self.continuous_gmms[column]\n",
    "            \n",
    "            # Extract normalized value and cluster probabilities\n",
    "            normalized = transformed_data[:, column_idx]\n",
    "            probs = transformed_data[:, column_idx + 1:column_idx + 1 + self.n_clusters]\n",
    "            \n",
    "            # Convert back to original space\n",
    "            cluster_idx = np.argmax(probs, axis=1)\n",
    "            values = np.zeros(len(normalized))\n",
    "            \n",
    "            for i in range(len(normalized)):\n",
    "                cluster = cluster_idx[i]\n",
    "                mean = gmm.means_[cluster][0]\n",
    "                std = np.sqrt(gmm.covariances_[cluster][0][0])\n",
    "                values[i] = normalized[i] * (4 * std) + mean\n",
    "                \n",
    "            result[column] = values\n",
    "            column_idx += self.n_clusters + 1\n",
    "            \n",
    "        return result\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_categories=0, hidden_dims=[256, 256]):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        dims = [input_dim + n_categories] + hidden_dims + [output_dim]\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:\n",
    "                self.layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "                self.layers.append(nn.LeakyReLU(0.2))\n",
    "                self.layers.append(nn.Dropout(0.2))\n",
    "                \n",
    "    def forward(self, noise, cond_vec=None):\n",
    "        if cond_vec is not None:\n",
    "            x = torch.cat([noise, cond_vec], dim=1)\n",
    "\n",
    "        else:\n",
    "            x = noise\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, n_categories=0, hidden_dims=[256, 128]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.pac = 1  # Default no pac\n",
    "        \n",
    "        # Placeholder for main layers - will be initialized in set_pac\n",
    "        self.main_layers = None\n",
    "        self.output_layer = None\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Store parameters for layer initialization\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_categories = n_categories\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Initialize layers with default pac=1\n",
    "        self._init_layers()\n",
    "            \n",
    "    def _init_layers(self):\n",
    "        \"\"\"Initialize network layers based on current pac value\"\"\"\n",
    "        pac_input_dim = self.input_dim * self.pac\n",
    "        \n",
    "        self.main_layers = nn.ModuleList()\n",
    "        for i in range(len(self.hidden_dims)):\n",
    "            if i == 0:\n",
    "                self.main_layers.append(nn.Linear(pac_input_dim, self.hidden_dims[i]))\n",
    "            else:\n",
    "                self.main_layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
    "            self.main_layers.append(nn.LeakyReLU(0.2))\n",
    "            self.main_layers.append(nn.Dropout(0.2))\n",
    "        self.main_layers.to(self.device)\n",
    "        self.output_layer = nn.Linear(self.hidden_dims[-1], 1, device= self.device)\n",
    "        \n",
    "        # Conditional embedding layers\n",
    "        self.cond_layers = None\n",
    "        if self.n_categories > 0:\n",
    "            \n",
    "            self.cond_layers = nn.Sequential(\n",
    "                nn.Linear(self.n_categories, pac_input_dim),\n",
    "                nn.ReLU()\n",
    "            ).to(self.device)\n",
    "    \n",
    "    def set_pac(self, pac):\n",
    "        \"\"\"Update the model to handle pac-sized inputs\"\"\"\n",
    "        self.pac = pac\n",
    "        self._init_layers()\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        if cond_vec is not None and self.cond_layers is not None:\n",
    "            cond = self.cond_layers(cond_vec)\n",
    "            x = x + cond\n",
    "            \n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class PacGan(nn.Module):\n",
    "    \"\"\"PacGAN discriminator for improved GAN training stability.\"\"\"\n",
    "    def __init__(self, discriminator, pac=10):\n",
    "        super(PacGan, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.pac = pac\n",
    "        # Tell the discriminator about pac to handle dimensions\n",
    "        if hasattr(self.discriminator, 'set_pac'):\n",
    "            self.discriminator.set_pac(pac)\n",
    "        \n",
    "    def forward(self, x, cond_vec=None):\n",
    "        batch_size = x.size(0)\n",
    "        if batch_size % self.pac != 0:\n",
    "            # Padding to make divisible by pac\n",
    "            pad_size = self.pac - (batch_size % self.pac)\n",
    "            indices = np.random.choice(batch_size, pad_size)\n",
    "            x = torch.cat([x, x[indices]], dim=0)\n",
    "            if cond_vec is not None:\n",
    "                cond_vec = torch.cat([cond_vec, cond_vec[indices]], dim=0)\n",
    "                \n",
    "        # Reshape x for PacGAN structure\n",
    "        new_batch_size = x.size(0) // self.pac\n",
    "        x_reshaped = x.view(new_batch_size, self.pac * x.size(1))\n",
    "        \n",
    "        # For conditional vectors, we need to have one per batch\n",
    "        if cond_vec is not None:\n",
    "            # Take one conditional vector per pac group\n",
    "            cond_vec_reshaped = cond_vec.view(new_batch_size, self.pac, cond_vec.size(1))\n",
    "            cond_vec_flat = cond_vec_reshaped[:, 0, :]  # Just take the first one\n",
    "            return self.discriminator(x_reshaped, cond_vec_flat)\n",
    "        else:\n",
    "            return self.discriminator(x_reshaped, None)\n",
    "\n",
    "class CTGAN:\n",
    "    def __init__(self, categorical_columns=None, noise_dim=100, batch_size=500, \n",
    "                 generator_lr=2e-4, discriminator_lr=2e-4, pac=10):\n",
    "        self.categorical_columns = categorical_columns if categorical_columns else []\n",
    "        self.noise_dim = noise_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.generator_lr = generator_lr\n",
    "        self.discriminator_lr = discriminator_lr\n",
    "        self.pac = pac\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.transformer = None\n",
    "        self.dataset = None\n",
    "        self.train = None\n",
    "        self.val = None\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        self.generator_optimizer = None\n",
    "        self.discriminator_optimizer = None\n",
    "    def fit(self, data, val_size= 0.2, epochs=300):\n",
    "        \"\"\"Fit CTGAN to the data with train and validation datasets and learning rate scheduling\"\"\"\n",
    "        # Create datasets\n",
    "        self.dataset = CTGANDataset(data, categorical_columns=self.categorical_columns)\n",
    "        categorical_dims = self.dataset.get_categorical_dims()\n",
    "        #########################################################################################################################################\n",
    "        train, val = self.dataset.train_val_split(val_ratio= 0.2)\n",
    "        \n",
    "        self.train = TransformedCTGANDataset(train, categorical_columns=self.categorical_columns, categorical_dims=categorical_dims)\n",
    "        self.val = TransformedCTGANDataset(val, categorical_columns=self.categorical_columns, categorical_dims=categorical_dims)\n",
    "        \n",
    "        # Use the same transformer for both datasets\n",
    "        self.transformer = self.dataset.transformer\n",
    "        \n",
    "        # Calculate dimensions\n",
    "        data_dim = self.dataset.transformed_data.shape[1]\n",
    "        n_categories = 0\n",
    "        if self.train.cond_vec:\n",
    "            n_categories = self.dataset.cond_vec.n_categories\n",
    "        \n",
    "        # Initialize models\n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim, \n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.generator_optimizer = optim.Adam(\n",
    "            self.generator.parameters(), \n",
    "            lr=self.generator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        self.discriminator_optimizer = optim.Adam(\n",
    "            self.discriminator.parameters(), \n",
    "            lr=self.discriminator_lr, \n",
    "            betas=(0.5, 0.9)\n",
    "        )\n",
    "        \n",
    "        # Setup learning rate schedulers with proper optimizer references\n",
    "        generator_scheduler = ReduceLROnPlateau(\n",
    "            self.generator_optimizer, \n",
    "            'min', \n",
    "            patience=10, \n",
    "            factor=0.2\n",
    "        )\n",
    "        \n",
    "        discriminator_scheduler = ReduceLROnPlateau(\n",
    "            self.discriminator_optimizer, \n",
    "            'min', \n",
    "            patience=10, \n",
    "            factor=0.2\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        # Ensure batch size is a multiple of pac for PacGAN\n",
    "        batch_size = (self.batch_size // self.pac) * self.pac\n",
    "        if batch_size == 0:\n",
    "            batch_size = self.pac\n",
    "            \n",
    "        run = wandb.init(project=\"Data Augmentation - CTGAN\", name=\"ctgan\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": self.batch_size,\n",
    "        \"generator_lr\": self.generator_lr,\n",
    "        \"discriminator_lr\": self.discriminator_lr,\n",
    "        \"pac\": self.pac,\n",
    "        \"noise_dim\": self.noise_dim,\n",
    "        \"categorical_columns\": self.categorical_columns,\n",
    "        \"data_dim\": data_dim,\n",
    "        \"n_categories\": n_categories\n",
    "    })\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.generator.train()\n",
    "            self.discriminator.train()\n",
    "            \n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            for _ in range(max(1, len(self.train) // batch_size)):\n",
    "                # Get real data and conditional vectors from training set\n",
    "                real_data, cond_vec, mask = self.train.sample(batch_size)\n",
    "                real_data = torch.from_numpy(real_data).to(self.device)\n",
    "                \n",
    "                if cond_vec is not None:\n",
    "                    \n",
    "                    #cond_vec = torch.from_numpy(cond_vec).to(self.device)\n",
    "                    cond_vec = cond_vec.to(self.device)\n",
    "                    #mask = torch.from_numpy(mask).to(self.device)\n",
    "                    mask = mask.to(self.device)\n",
    "                \n",
    "                # Labels for real and fake data\n",
    "                real_labels = torch.ones(batch_size // self.pac, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size // self.pac, 1).to(self.device)\n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.discriminator_optimizer.zero_grad()\n",
    "                \n",
    "                # Real data loss\n",
    "                outputs = self.discriminator(real_data, cond_vec)\n",
    "                d_real_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                # Generate fake data\n",
    "                noise = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
    "                fake_data = self.generator(noise, cond_vec)\n",
    "                \n",
    "                outputs = self.discriminator(fake_data.detach(), cond_vec)\n",
    "                d_fake_loss = criterion(outputs, fake_labels)\n",
    "                \n",
    "                d_loss = d_real_loss + d_fake_loss\n",
    "                d_loss.backward()\n",
    "                self.discriminator_optimizer.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                self.generator_optimizer.zero_grad()\n",
    "                \n",
    "                outputs = self.discriminator(fake_data, cond_vec)\n",
    "                g_loss = criterion(outputs, real_labels)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                self.generator_optimizer.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # Validation phase\n",
    "            self.generator.eval()\n",
    "            self.discriminator.eval()\n",
    "            \n",
    "            g_val_losses = []\n",
    "            d_val_losses = []\n",
    "            d_val_real_losses = []\n",
    "            d_val_fake_losses = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(max(1, len(self.val) // batch_size)):\n",
    "                    # Get real data and conditional vectors from validation set\n",
    "                    real_val_data, val_cond_vec, val_mask = self.val.sample(batch_size)\n",
    "                    real_val_data = torch.from_numpy(real_val_data).to(self.device)\n",
    "                    \n",
    "                    if val_cond_vec is not None:\n",
    "                        #val_cond_vec = torch.from_numpy(val_cond_vec).to(self.device)\n",
    "                        #val_mask = torch.from_numpy(val_mask).to(self.device)\n",
    "                        val_cond_vec = val_cond_vec.to(self.device)\n",
    "                        val_mask = val_mask.to(self.device)\n",
    "                        \n",
    "                    \n",
    "                    # Labels for real and fake validation data\n",
    "                    real_labels = torch.ones(batch_size // self.pac, 1).to(self.device)\n",
    "                    fake_labels = torch.zeros(batch_size // self.pac, 1).to(self.device)\n",
    "                    \n",
    "                    # Validation - Discriminator\n",
    "                    val_outputs = self.discriminator(real_val_data, val_cond_vec)\n",
    "                    d_val_real_loss = criterion(val_outputs, real_labels)\n",
    "                    d_val_real_losses.append(d_val_real_loss.item())\n",
    "                    \n",
    "                    # Generate fake validation data\n",
    "                    val_noise = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
    "                    val_fake_data = self.generator(val_noise, val_cond_vec)\n",
    "                    \n",
    "                    val_outputs = self.discriminator(val_fake_data, val_cond_vec)\n",
    "                    d_val_fake_loss = criterion(val_outputs, fake_labels)\n",
    "                    d_val_fake_losses.append(d_val_fake_loss.item())\n",
    "                    \n",
    "                    d_val_loss = d_val_real_loss + d_val_fake_loss\n",
    "                    \n",
    "                    # Validation - Generator\n",
    "                    g_val_loss = criterion(val_outputs, real_labels)\n",
    "                    \n",
    "                    g_val_losses.append(g_val_loss.item())\n",
    "                    d_val_losses.append(d_val_loss.item())\n",
    "            \n",
    "            # Calculate average losses\n",
    "            avg_g_loss = np.mean(g_losses)\n",
    "            avg_d_loss = np.mean(d_losses)\n",
    "            avg_g_val_loss = np.mean(g_val_losses)\n",
    "            avg_d_val_loss = np.mean(d_val_losses)\n",
    "            avg_d_val_real_loss = np.mean(d_val_real_losses)\n",
    "            avg_d_val_fake_loss = np.mean(d_val_fake_losses)\n",
    "            \n",
    "            # Update learning rates based on validation losses\n",
    "            generator_scheduler.step(avg_g_val_loss)\n",
    "            discriminator_scheduler.step(avg_d_val_loss)\n",
    "            \n",
    "            # Get current learning rates\n",
    "            g_lr = self.generator_optimizer.param_groups[0]['lr']\n",
    "            d_lr = self.discriminator_optimizer.param_groups[0]['lr']\n",
    "            wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"g_loss\": avg_g_loss,\n",
    "            \"d_loss\": avg_d_loss,\n",
    "            \"g_val_loss\": avg_g_val_loss,\n",
    "            \"d_val_loss\": avg_d_val_loss,\n",
    "            \"d_val_real_loss\": avg_d_val_real_loss,\n",
    "            \"d_val_fake_loss\": avg_d_val_fake_loss,\n",
    "            \"generator_lr\": g_lr,\n",
    "            \"discriminator_lr\": d_lr\n",
    "        })\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "                      f\"Train - G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}, \"\n",
    "                      f\"Val - G Loss: {avg_g_val_loss:.4f}, D Loss: {avg_d_val_loss:.4f}, \"\n",
    "                      f\"LR - Generator: {g_lr:.6f}, Discriminator: {d_lr:.6f}\")\n",
    "        wandb.finish()\n",
    "        save = input(\"Save? 0 is not save, other is save\")\n",
    "        is_saved = False\n",
    "        if save:\n",
    "            file_name = input(\"Tap model's name:\" )\n",
    "            while not is_saved:\n",
    "                if os.path.exists(f\"models/{file_name}.pth\"):\n",
    "                    print(\"Model's name existes!!!!\")\n",
    "                    file_name = input(\"Tap model's name:\" )\n",
    "                else:\n",
    "                    self.save(f\"models/{file_name}.pth\")\n",
    "                    is_saved = True\n",
    "                    print(\"Done!!!!\")\n",
    "            return 1\n",
    "    def generate(self, n_samples, conditions= None):\n",
    "        \"\"\"Generate synthetic samples with optional conditioning.\"\"\"\n",
    "        if self.generator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        self.generator.eval()\n",
    "        \n",
    "        steps = n_samples // self.batch_size + 1\n",
    "        data = []\n",
    "        \n",
    "        for i in range(steps):\n",
    "            n_batch = min(self.batch_size, n_samples - i * self.batch_size)\n",
    "            if n_batch <= 0:\n",
    "                break\n",
    "                \n",
    "            # Generate noise\n",
    "            noise = torch.randn(n_batch, self.noise_dim).to(self.device)\n",
    "            \n",
    "            # Generate conditional vector if necessary\n",
    "            cond_vec = None\n",
    "            if self.dataset.cond_vec and conditions:\n",
    "                cond_vec = self.dataset.cond_vec.generate_conditional_vector(conditions, n_batch)\n",
    "                cond_vec = cond_vec.to(self.device)\n",
    "                \n",
    "            # Generate data\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                fake = self.generator(noise, cond_vec)\n",
    "            data.append(fake.cpu().numpy())\n",
    "            \n",
    "        data = np.concatenate(data, axis=0)\n",
    "        \n",
    "        # Convert to the original data format\n",
    "        synthetic_data = self.transformer.inverse_transform(data[:n_samples])\n",
    "        return synthetic_data\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        if self.generator is None or self.discriminator is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        \"\"\"state = {\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'transformer': self.transformer\n",
    "        }\n",
    "        \n",
    "        torch.save(state, path)\"\"\"\n",
    "        checkpoint = {\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'generator_optimizer': self.generator_optimizer.state_dict(),\n",
    "            'discriminator_optimizer': self.discriminator_optimizer.state_dict(),\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'transformer': self.transformer  \n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        state = torch.load(path, map_location=self.device)\n",
    "        self.generator_optimizer = state['generator_optimizer']\n",
    "        self.discriminator_optimizer = state['discriminator_optimizer']\n",
    "        self.noise_dim = state['noise_dim']\n",
    "        self.categorical_columns = state['categorical_columns']\n",
    "        self.transformer = state['transformer']\n",
    "        \n",
    "        # Recreate the dataset and models\n",
    "        n_categories = 0\n",
    "        data_dim = 0\n",
    "        \n",
    "        if hasattr(self.transformer, 'categorical_dims'):\n",
    "            n_categories = sum(self.transformer.categorical_dims.values())\n",
    "            if hasattr(self.transformer, 'continuous_gmms'):\n",
    "                continuous_dims = sum([gmm.n_components + 1 for gmm in self.transformer.continuous_gmms.values()])\n",
    "                data_dim = n_categories + continuous_dims\n",
    "        \n",
    "        self.generator = Generator(\n",
    "            input_dim=self.noise_dim,\n",
    "            output_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        discriminator = Discriminator(\n",
    "            input_dim=data_dim,\n",
    "            n_categories=n_categories\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.discriminator = PacGan(discriminator, pac=self.pac)\n",
    "        \n",
    "        self.generator.load_state_dict(state['generator'])\n",
    "        self.discriminator.load_state_dict(state['discriminator'])\n",
    "        \n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc59e6-70a4-4b52-9af6-beef7f2bfae3",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db188c46-467a-4aaa-a35a-1690d508e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\anhqu\\Desktop\\Anh Quan\\Data Augmentation\\Code\\wandb\\run-20250325_193904-en27783e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/en27783e' target=\"_blank\">ctgan</a></strong> to <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/en27783e' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/en27783e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train - G Loss: 2.1219, D Loss: 1.2687, Val - G Loss: 1.6256, D Loss: 0.8865, LR - Generator: 0.005000, Discriminator: 0.002000\n",
      "Epoch [10/50], Train - G Loss: 1.0850, D Loss: 1.2874, Val - G Loss: 0.8279, D Loss: 1.3237, LR - Generator: 0.005000, Discriminator: 0.002000\n",
      "Epoch [20/50], Train - G Loss: 0.8656, D Loss: 1.2908, Val - G Loss: 0.6986, D Loss: 1.3241, LR - Generator: 0.001000, Discriminator: 0.000400\n",
      "Epoch [30/50], Train - G Loss: 0.7193, D Loss: 1.4109, Val - G Loss: 0.6391, D Loss: 1.4465, LR - Generator: 0.000200, Discriminator: 0.000080\n",
      "Epoch [40/50], Train - G Loss: 0.6762, D Loss: 1.4366, Val - G Loss: 0.6359, D Loss: 1.4285, LR - Generator: 0.000040, Discriminator: 0.000016\n",
      "Epoch [50/50], Train - G Loss: 0.6883, D Loss: 1.4159, Val - G Loss: 0.6445, D Loss: 1.4157, LR - Generator: 0.000008, Discriminator: 0.000003\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>▄▁▅█▇▆▅▅▅▄▃▃▃▆▅▅▄▄▅▄█▆▅▇▇▆▆▆▇██▇▇▇▇▇▇▇▇▇</td></tr><tr><td>d_val_fake_loss</td><td>▁▁█▆▄▄▃▅▄▅▅▃▅▄▃▅▅▄▄▅▅▄▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>d_val_loss</td><td>▂▁█▆▅▅▄▄▅▄▃▅▄▄▄▄▄▄▄▆▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>d_val_real_loss</td><td>▅▁▇▄▇█▆▆▆▂▂▄▅▄▂▄▂▂▅▄▄▅▅▆▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>discriminator_lr</td><td>█████████▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>g_loss</td><td>▅█▃▂▁▁▂▂▂▂▃▂▂▁▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>g_val_loss</td><td>▇█▁▂▂▃▅▂▃▃▃▂▃▄▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>generator_lr</td><td>██████████▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>d_loss</td><td>1.41585</td></tr><tr><td>d_val_fake_loss</td><td>0.75154</td></tr><tr><td>d_val_loss</td><td>1.41568</td></tr><tr><td>d_val_real_loss</td><td>0.66414</td></tr><tr><td>discriminator_lr</td><td>0.0</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>g_loss</td><td>0.68826</td></tr><tr><td>g_val_loss</td><td>0.64454</td></tr><tr><td>generator_lr</td><td>1e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ctgan</strong> at: <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/en27783e' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN/runs/en27783e</a><br> View project at: <a href='https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN' target=\"_blank\">https://wandb.ai/tuanhquanle-insa-toulouse/Data%20Augmentation%20-%20CTGAN</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250325_193904-en27783e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Save? 0 is not save, other is save 1\n",
      "Tap model's name: ctgan0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!!\n",
      "\n",
      "Real data statistics:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   MedHouseVal  HouseAge_Cat  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558      2.196560  \n",
      "std       10.386050      2.135952      2.003532      1.153956      1.229386  \n",
      "min        0.692308     32.540000   -124.350000      0.149990      0.000000  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000      1.000000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000      2.000000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250      3.000000  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010      4.000000  \n",
      "\n",
      "Conditioned data (HouseAge_Cat = 2) statistics:\n",
      "       HouseAge_Cat    MedInc   HouseAge  AveRooms  AveBedrms   Population  \\\n",
      "count      7.000000  7.000000   7.000000  7.000000   7.000000     7.000000   \n",
      "mean       1.857143  4.559869  25.712112  5.212113   1.048927  1111.099864   \n",
      "std        0.899735  1.878387   9.230176  0.972220   0.028853   399.500380   \n",
      "min        1.000000  1.399585  15.751390  3.849211   1.004746   795.122863   \n",
      "25%        1.000000  3.734565  17.358062  4.732216   1.035669   859.672568   \n",
      "50%        2.000000  5.433538  27.311401  5.163036   1.037435   863.712106   \n",
      "75%        2.500000  5.631524  32.444025  5.551259   1.069729  1300.888653   \n",
      "max        3.000000  6.353780  37.317818  6.905594   1.089514  1797.741634   \n",
      "\n",
      "       AveOccup   Latitude   Longitude  MedHouseVal  \n",
      "count  7.000000   7.000000    7.000000     7.000000  \n",
      "mean   3.527093  34.919680 -118.299835     2.467618  \n",
      "std    0.909380   1.737980    1.072119     1.344877  \n",
      "min    2.541014  33.660422 -120.279152     0.735734  \n",
      "25%    3.008984  33.890008 -118.672380     1.770319  \n",
      "50%    3.388466  34.053908 -118.198027     2.327851  \n",
      "75%    3.696198  35.569768 -117.527834     2.943051  \n",
      "max    5.349806  37.803876 -117.221239     4.783003  \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "def example_usage():\n",
    "    # Sample tabular data\n",
    "\n",
    "    \n",
    "    # Load data\n",
    "    data = fetch_california_housing(as_frame=True).frame\n",
    "    \n",
    "    # For this example, let's convert HouseAge to categorical by binning\n",
    "    data['HouseAge_Cat'] = pd.cut(data['HouseAge'], bins=5, labels=False)\n",
    "    categorical_columns = ['HouseAge_Cat']\n",
    "    # Initialize CTGAN\n",
    "    ctgan = CTGAN(categorical_columns=categorical_columns, pac=5, generator_lr=0.005, discriminator_lr=0.002, batch_size= 500)  # Ajusté le pac à 5 pour éviter des problèmes de dimensionnalité\n",
    "    \n",
    "    # Fit model\n",
    "    print(\"Training CTGAN model...\")\n",
    "    \n",
    "    ctgan.fit(data , epochs=50)\n",
    "    \n",
    "    # Generate synthetic data (unconditional)\n",
    "    #print(\"Generating synthetic data...\")\n",
    "    #synthetic_data = ctgan.generate(n_samples=1000)\n",
    "    \n",
    "    # Generate synthetic data with conditions\n",
    "    # Example condition: HouseAge_Cat = 2\"\"\"\n",
    "    conditioned_data = ctgan.generate(\n",
    "        n_samples=7,\n",
    "        conditions={'HouseAge_Cat': 2}\n",
    "    )\n",
    "    \n",
    "    # Compare statistics\n",
    "    print(\"\\nReal data statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    #print(\"\\nSynthetic data statistics:\")\n",
    "    #print(synthetic_data.describe())\n",
    "    \n",
    "    print(\"\\nConditioned data (HouseAge_Cat = 2) statistics:\")\n",
    "    print(conditioned_data.describe())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77bd8342-b732-4dc3-9a9a-8707086462a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_california_housing(as_frame=True).frame\n",
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Environment",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
